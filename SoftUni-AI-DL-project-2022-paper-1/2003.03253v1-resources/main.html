<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Lihi Shiloh-Perl and Raja Giryes" />
  <title>Introduction to deep learning</title>
  <style type="text/css">code{white-space: pre;}</style>
</head>
<body>
<div id="header">
<h1 class="title">Introduction to deep learning</h1>
<h2 class="author">Lihi Shiloh-Perl and Raja Giryes</h2>
</div>
<h1 id="sec:basic_overview">General overview</h1>
<p>Neural Networks (NN) have revolutionized the modern day-to-day life. Their significant impact is present even in our most basic actions, such as ordering products on-line via Amazon‚Äôs Alexa or passing the time with on-line video games against computer agents. The NN effect is evident in many more occasions, for example, in medical imaging NNs are utilized for lesion detection and segmentation¬†<span class="citation"></span>, and tasks such as text-to-speech¬†<span class="citation"></span> and text-to-image¬†<span class="citation"></span> have remarkable improvements thanks to this technology. In addition, the advancements they have caused in fields such as natural language processing (NLP)¬†<span class="citation"></span>, optics <span class="citation"></span>, image processing <span class="citation"></span> and computer vision (CV)¬†<span class="citation"></span> are astonishing, creating a leap forward in technology such as autonomous driving¬†<span class="citation"></span>, face recognition¬†<span class="citation"></span>, anomaly detection¬†<span class="citation"></span>, text understanding¬†<span class="citation"></span> and art¬†<span class="citation"></span>, to name a few. Its influence is powerful and is continuing to grow.</p>
<p>The NN journey began in the mid 1960‚Äôs with the publication of the Perceptron <span class="citation"></span>. Its development was motivated by the formulation of the human neuron activity <span class="citation"></span> and research regarding the human visual perception <span class="citation"></span>. However, quite quickly, a deceleration in the field was experienced, which lasted for almost three decades. This was mainly the result of lack of theory with respect to the possibility of training the (single-layer) perceptron and a series of theoretical results that emphasized its limitations, where the most remarkable one is its inability to learn the XOR function <span class="citation"></span>.</p>
<p>This <em>NN ice age</em> came to a halt in the mid 1980‚Äôs, mainly with the introduction of the multi-layer perceptron (MLP) and the backpropagation algorithm <span class="citation"></span>. Furthermore, the revolutionary convolutional layer was presented <span class="citation"></span>, where one of its notable achievements was successfully recognizing hand-written digits <span class="citation"></span>.</p>
<p>While some other significant developments have happened in the following decade, such as the development of the long-short memory machine (LSTM) <span class="citation"></span>, the field experienced another deceleration. Questions were arising with no adequate answers especially with respect to the non-convex nature of the used optimization objectives, overfitting the training data, and the challenge of vanishing gradients. These difficulties led to a second <em>NN winter</em>, which lasted two decades. In the meantime, classical machine learning techniques were developed and attracted much academic and industry attention. One of the prominent algorithms was the newly proposed Support Vector Machine (SVM) <span class="citation"></span>, which defined a convex optimization problem with a clear mathematical interpretation¬†<span class="citation"></span>. These properties increased its popularity and usage in various applications.</p>
<p>The <span class="math inline">21<sup>st</sup></span> century began with some advancements in neural networks in the areas of speech processing and Natural Language Processing (NLP). Hinton <em>et al.</em>¬†<span class="citation"></span> proposed a method for layer-wise initial training of neural networks, which leveraged some of the challenges in training networks with several layers. However, the great NN <em>tsunami</em> truly hit the field with the publication of <em>AlexNet</em> in 2012¬†<span class="citation"></span>. In this paper, Krizhevsky <em>et al.</em> presented a neural network that achieved state-of-the-art performance on the ImageNet¬†<span class="citation"></span> challenge, where the goal is to classify images into 1000 categories using 1.2 Million images for training and 150000 images for testing. The improvement over the runner-up, which relied on hand crafted features and one of the best classification techniques of that time, was notable - more than <span class="math inline">10%</span>. This caused the whole research community to understand that neural networks are way more powerful than what was thought and they bear a great potential for many applications. This led to a myriad of research works that applied NNs for various fields showing their great advantage.</p>
<p>Nowadays, it is safe to say that almost every research field has been affected by this NN <em>tsunami</em> wave, experiencing significant improvements in abilities and performance. Many of the tools used today are very similar to the ones used in the previous phase of NN. Indeed, some new regularization techniques such as batch-normalization¬†<span class="citation"></span> and dropout¬†<span class="citation"></span> have been proposed. Yet, the key-enablers for the current success is the large amounts of data available today that are essential for large NN training, and the developments in GPU computations that accelerate the training time significantly (sometimes even leading to <span class="math inline">√ó100</span> speed-up compared to training on a conventional CPU). The advantages of NN is remarkable especially at large scales. Thus, having large amounts of data and the appropriate hardware to process them, is vital for their success.</p>
<p>A major example of a tool that did not exist before is the Generative Adversarial Network (GAN¬†<span class="citation"></span>). In 2014, Goodfellow <em>et al.</em> published this novel framework for learning data distribution. The framework is composed of two models, a generator and a discriminator, trained as adversaries. The generator is trained to capture the data distribution, while the discriminator is trained to differentiate between generated (‚Äúfake‚Äù) data and real data. The goal is to let the generator synthesize data, which the discriminator fails to discriminate from the real one. The GAN architecture is used in more and more applications since its introduction in 2014. One such application is the rendering of real scene images were GANs have proved very successful¬†<span class="citation"></span>. For example, Brock <em>et al.</em> introduced the BigGAN¬†<span class="citation"></span> architecture that exhibited impressive results in creating high-resolution images, shown in Fig.¬†[fig:BigGAN_example1]. While most GAN techniques learn from a set of images, recently it has been successfully demonstrated that one may even train a GAN just using one image¬†<span class="citation"></span>. Other GAN application include inpainting <span class="citation"></span>, retargeting¬†<span class="citation"></span>, 3D modeling <span class="citation"></span>, semi-supervised learning¬†<span class="citation"></span>, domain adaptation¬†<span class="citation"></span> and more.</p>
<div class="figure">
<img src="pics/BigGAN_example1.png" alt="Class-conditional samples generated by a GAN, ." style="width:75.0%" />
<p class="caption">Class-conditional samples generated by a GAN, <span class="citation"></span>.<span data-label="fig:BigGAN_example1"></span></p>
</div>
<p>While neural networks are very successful, the theoretical understanding behind them is still missing. In this respect, there are research efforts that try to provide a mathematical formulation that explains various aspects of NN. For example, they study NN properties such as their optimization¬†<span class="citation"></span>, generalization <span class="citation"></span> and expressive power <span class="citation"></span>.</p>
<p>The rest of the chapter is organized as follows. In Section¬†[sec:basic_structure] the basic structure of a NN is described, followed by details regarding popular loss functions and metric learning techniques used today (Section¬†[sec:LF]). We continue with an introduction to the NN training process in Section¬†[sec:training], including a mathematical derivation of backpropagation and training considerations. Section¬†[sec:optimizers] elaborates on the different optimizers used during training, after which Section¬†[sec:regularizations] presents a review of common regularization schemes. Section¬†[sec:architectures] details advanced NN architecture with state-of-the-art performances and Section¬†[sec:summary] concludes the chapter by highlighting some current important NN challenges.</p>
<h1 id="sec:basic_structure">Basic NN structure</h1>
<p>The basic building block of a NN consists of a linear operation followed by a non-linear function. Each building block consists of a set of parameters, termed weights and biases (sometimes the term weights includes also the biases), that are updated in the training process with the goal of minimizing a pre-defined loss function.</p>
<p>Assume an input data <span class="math inline"><strong>x</strong>‚ÄÑ‚àà‚ÄÑ‚Ñù<sup><em>d</em><sub>0</sub></sup></span>, the output of the building block is of the form , where <span class="math inline"><em>œà</em>(‚ãÖ)</span> is a non-linear function, <span class="math inline"><strong>W</strong>‚ÄÑ‚àà‚ÄÑ‚Ñù<sup><em>d</em><sub>1</sub>‚ÄÖ√ó‚ÄÖ<em>d</em><sub>0</sub></sup></span> is the linear operation and <span class="math inline"><strong>b</strong>‚ÄÑ‚àà‚ÄÑ‚Ñù<sup><em>d</em><sub>1</sub></sup></span> is the bias. See Fig.¬†[fig:building_block] for an illustration of a single building block.</p>
<div class="figure">
<img src="pics/building_block.png" alt="NN building block consists of a linear and a non-linear elements. The weights \mathbf{W} and biases \mathbf{b} are the parameters of the layer." style="width:50.0%" />
<p class="caption">NN building block consists of a linear and a non-linear elements. The weights <span class="math inline"><strong>W</strong></span> and biases <span class="math inline"><strong>b</strong></span> are the parameters of the layer.<span data-label="fig:building_block"></span></p>
</div>
<div class="figure">
<img src="pics/NN_illustraion.png" alt="NN layered structure: concatenation of N building blocks, e.g., model layers." />
<p class="caption">NN layered structure: concatenation of <span class="math inline"><em>N</em></span> building blocks, e.g., model layers.<span data-label="fig:NN_illustraion"></span></p>
</div>
<p>To form an NN model, such building blocks are concatenated one to another in a layered structure that allows the input data to be gradually processed as it propagates through the network. Such a process is termed the (feed-)forward pass. Following it, during training, a backpropagation process is used to update the NN parameters, as elaborated in Section¬†[subsec:backprop]. In inference time, only the forward pass is used.</p>
<p>Fig.¬†[fig:NN_illustraion] illustrates the concatenation of <span class="math inline"><em>K</em></span> building blocks, e.g., layers. The intermediate output at the end of the model (before the ‚Äútask driven block‚Äù) is termed the <em>network embedding</em> and it is formulated as follows: <br /><span class="math display">$$\resizebox{.92 \textwidth}{!}{$
    \Phi(\mathbf{x},\mathbf{W}^{(1)},...,\mathbf{W}^{(K)},\mathbf{b}^{(1)},...,\mathbf{b}^{(K)})=\psi(\mathbf{W}^{(K)}...\psi(\mathbf{W}^{(2)}\psi(\mathbf{W}^{(1)}\mathbf{x}+\mathbf{b}^{(1)})+\mathbf{b}^{(2)})...+\mathbf{b}^{(K)}).
    $}$$</span><br /> The final output (prediction) of the network is estimated from the network embedding of the input data using an additional task driven layer. A popular example is the case of classifications, where this block is usually a linear operation followed by the <em>cross-entropy</em> loss function (detailed in Section¬†[sec:LF]).</p>
<p>When approaching the analysis of data with varying length, such as sequential data, a variant of the aforementioned approach is used. A very popular example for such a neural network structure is the Recurrent Neural Network (RNN¬†<span class="citation"></span>). In a vanilla RNN model, the network receives at each time step just a single input but with a feedback loop calculated using the result of the same network in the previous time-step (see an illustration in Fig.¬†[fig:RNN]). This enables the network to ‚Äúremember‚Äù information and support multiple inputs and producing one or more outputs.</p>
<p>More complex RNN structures include performing bi-directional calculations or adding gating to the feedback and the input received by the network. The most known complex RNN architecture is the Long-Term-Short-Memory (LSTM)¬†<span class="citation"></span>, which adds gates to the RNN. These gates decide what information from the current input and the past will be used to calculate the output and the next feedback, as well as what information to mask (i.e., causing the network to forget). This enables an easier combination of past and present information. It is commonly used for time-series data in domains such as NLP and speech processing.</p>
<div class="figure">
<img src="pics/RNN_series.png" alt="Recurrent NN (RNN) illustration for time series data. The feedback loop introduces time dependent characteristics to the NN model using an element-wise function. The weights are the same along all time steps." style="width:35.0%" />
<p class="caption">Recurrent NN (RNN) illustration for time series data. The feedback loop introduces time dependent characteristics to the NN model using an element-wise function. The weights are the same along all time steps.<span data-label="fig:RNN"></span></p>
</div>
<p>Another common network structure is the <em>Encoder-Decoder</em> architecture. The first part of the model, the encoder, reduces the dimensions of the input to a compact feature vector. This vector functions as the input to the second part of the model, the decoder. The decoder increases its dimension, usually, back to the original input size. This architecture essentially learns to compress (encode) the input to an efficiently small vector and then decode the information from its compact representation. In the context of regular feedforward NN, this model is known as autoencoder¬†<span class="citation"></span> and is used for several tasks such as image denoising <span class="citation"></span>, image captioning <span class="citation"></span>, feature extraction¬†<span class="citation"></span> and segmentation¬†<span class="citation"></span>. In the context of sequential data, it is used for tasks such as translation, where the decoder generates a translated sentence from a vector representing the input sentence <span class="citation"></span>.</p>
<h2 id="sec:layers">Common linear layers</h2>
<p>A common basic NN building block is the Fully Connected (FC) layer. A network composed of a concatenation of such layers is termed Multi-Layer Perceptron (MLP)¬†<span class="citation"></span>. The FC layer connects every neuron in one layer to every neuron in the following layer, i.e. the matrix <span class="math inline"><strong>W</strong></span> is dense. It enables information propagation from all neurons to all the ones following them. However it may not maintain spatial information. Figure¬†[fig:MLP] illustrates a network with FC layers.</p>
<div class="figure">
<img src="pics/MLP.png" alt="Fully-connected layers." style="width:50.0%" />
<p class="caption">Fully-connected layers.<span data-label="fig:MLP"></span></p>
</div>
<p>The convolutional layer¬†<span class="citation"></span> is another very common layer. We discuss here the 2D case, where the extension to other dimension is straight-forward. This layer applies one or multiple convolution filters to its input with kernels of size <span class="math inline"><em>W</em>‚ÄÖ√ó‚ÄÖ<em>H</em></span>. The output of the convolution layer is commonly termed a <em>feature map</em>.</p>
<p>Each neuron in a feature map receives inputs from a set of neurons from the previous layer, located in a small neighborhood defined by the kernel size. If we apply this relationship recursively, we can find the part of the input that affects each neuron at a given layer, i.e., the area of visible context that each neuron sees from the input. The size of this part is called the <em>receptive field</em>. It impacts the type and size of visual features each convolution layer may extract, such as edges, corners and even patterns. Since convolution operations maintain spatial information and are translation equivariant, they are very useful, namely, in image processing and CV.</p>
<p>If the input to a convolution layer has some arbitrary third dimension, for example 3-channels in an RGB image (<span class="math inline"><em>C</em>‚ÄÑ=‚ÄÑ3</span>) or some <span class="math inline"><em>C</em>‚ÄÑ&gt;‚ÄÑ1</span> channels from an output of a hidden layer in the model, the kernel of the matching convolution layer should be of size <span class="math inline"><em>W</em>‚ÄÖ√ó‚ÄÖ<em>H</em>‚ÄÖ√ó‚ÄÖ<em>C</em></span>. This corresponds to applying a different convolution for each input channel separately, and then summing the outputs to create one feature map. The convolution layer may create a multi-channel feature map by applying multiple filters to the input, i.e., using a kernel of size , where <span class="math inline"><em>C</em><sub>in</sub></span> and <span class="math inline"><em>C</em><sub>out</sub></span> are the number of channels at the input and output of the layer respectively.</p>
<h2 id="sec:AF">Common non-linear functions</h2>
<p>The non-linear functions defined for each layer are of great interest since they introduce the non-linear property to the model and can limit the propagating gradient from vanishing or exploding (see Section¬†[sec:training]).</p>
<p>Non-linear functions that are applied element-wise are known as <em>activation functions</em>. Common activation functions are the Rectified Linear Unit (ReLU <span class="citation"></span>), leaky ReLU <span class="citation"></span>, Exponential Linear Unit (ELU) <span class="citation"></span>, hyperbolic tangent (tanh) and sigmoid. There is no universal rule for choosing a specific activation function, however, ReLUs and ELUs are currently more popular for image processing and CV while sigmoid and tanh are more common in speech and NLP. Fig.¬†[fig:activation_funcs] presents the response of the different activation functions and Table [table:activation_functions] their mathematical formulation.</p>
<div class="figure">
<img src="pics/activation_functions.jpg" alt="Different activation functions. Leaky ReLU with \alpha=0.1, ELU with \alpha=1." style="width:65.0%" />
<p class="caption">Different activation functions. Leaky ReLU with <span class="math inline"><em>Œ±</em>‚ÄÑ=‚ÄÑ0.1</span>, ELU with <span class="math inline"><em>Œ±</em>‚ÄÑ=‚ÄÑ1</span>.<span data-label="fig:activation_funcs"></span></p>
</div>
<p><span>|c|c|c|c|</span></p>
<hr />
<hr />
<p>Function &amp; Formulation <span class="math inline"><em>s</em>(<em>x</em>)</span> &amp; Derivative <span class="math inline">$\frac{ds(x)}{dx}$</span> &amp; Function output range<br />
</p>
<hr />
<hr />
<p>ReLU&amp; <span class="math inline">$\begin{cases}
        0, &amp; \text{for } x&lt;0\\ 
        x, &amp; \text{for } x\geq 0
        \end{cases}$</span> &amp; <span class="math inline">$\begin{cases}
        0, &amp; \text{for } x&lt;0\\ 
        1, &amp; \text{for } x\geq 0
        \end{cases}$</span> &amp; <span class="math inline">[0,‚ÄÜ‚àû)</span><br />
</p>
<hr />
<hr />
<p>Leaky ReLU&amp; <span class="math inline">$\begin{cases}
        \alpha x, &amp; \text{for } x&lt;0\\ 
        x, &amp; \text{for } x\geq 0
        \end{cases}$</span> &amp; <span class="math inline">$\begin{cases}
        \alpha, &amp; \text{for } x&lt;0\\ 
        1, &amp; \text{for } x\geq 0
        \end{cases}$</span>&amp; <span class="math inline">(‚ÄÖ‚àí‚ÄÖ‚àû,‚ÄÜ‚àû)</span><br />
</p>
<hr />
<hr />
<p>ELU&amp; <span class="math inline">$\begin{cases}
        \alpha(\mathrm{e}^{x}-1), &amp; \text{for } x&lt;0\\ 
        x, &amp; \text{for } x\geq 0
        \end{cases}$</span>&amp; <span class="math inline">$\begin{cases}
        \alpha \mathrm{e}^{x}, &amp; \text{for } x&lt;0\\ 
        1, &amp; \text{for } x\geq 0
        \end{cases}$</span>&amp; <span class="math inline">[‚ÄÖ‚àí‚ÄÖ<em>Œ±</em>,‚ÄÜ‚àû)</span><br />
</p>
<hr />
<hr />
<p>Sigmoid&amp; <span class="math inline">$\frac{1}{1+\mathrm{e}^{-x}}$</span> &amp; <span class="math inline">$\frac{\mathrm{e}^{-x}}{(1+\mathrm{e}^{-x})^2}$</span> &amp; <span class="math inline">(0,‚ÄÜ1)</span><br />
</p>
<hr />
<hr />
<p>tanh&amp; <span class="math inline">$\tanh(x)=\frac{\mathrm{e}^{2x}-1}{\mathrm{e}^{2x}+1}$</span> &amp; <span class="math inline">1‚ÄÖ‚àí‚ÄÖtanh<sup>2</sup>(<em>x</em>)</span>&amp; <span class="math inline">(‚ÄÖ‚àí‚ÄÖ1,‚ÄÜ1)</span><br />
</p>
<p>Another common non-linear operations in a NN model are the <em>pooling</em> functions. They are aggregation operations that reduce dimensionality while keeping dominant features. Assume a pooling size of <span class="math inline"><em>q</em></span> and an input vector to a hidden layer of size <span class="math inline"><em>d</em></span>, <span class="math inline"><strong>z</strong>‚ÄÑ=‚ÄÑ[<em>z</em><sub>1</sub>,‚ÄÜ<em>z</em><sub>1</sub>,‚ÄÜ...,‚ÄÜ<em>z</em><sub><em>d</em></sub>]</span>. For every <span class="math inline"><em>m</em>‚ÄÑ‚àà‚ÄÑ[1,‚ÄÜ<em>d</em>]</span>, the subset of the input vector <span class="math inline">$\mathbf{\tilde{z}}=[z_m,z_{m+1},...,z_{q+m}]$</span> may undergo one of the following popular pooling operations:</p>
<ol>
<li><p>Max pooling: <span class="math inline">$g(\mathbf{\tilde{z}})=\max_i \mathbf{\tilde{z}}$</span></p></li>
<li><p>Mean pooling: <span class="math inline">$g(\mathbf{\tilde{z}})=\frac{1}{q}\sum_{i=m}^{q+m}z_i$</span></p></li>
<li><p><span class="math inline">‚Ñì<sub><em>p</em></sub></span> pooling: <span class="math inline">$g(\mathbf{\tilde{z}})=\sqrt[p]{\sum_{i=m}^{q+m} z^p_i}$</span></p></li>
</ol>
<p>All pooling operations are characterized by a stride, <span class="math inline"><em>s</em></span>, that effectively defines the output dimensions. Applying pooling with a stride <span class="math inline"><em>s</em></span>, is equivalent to applying the pooling with no stride (i.e., <span class="math inline"><em>s</em>‚ÄÑ=‚ÄÑ1</span>) and then sub-sampling by a factor of <span class="math inline"><em>s</em></span>. It is common to add zero padding to <span class="math inline"><strong>z</strong></span> such that its length is divisible by <span class="math inline"><em>s</em></span>.</p>
<p>Another very common non-linear function is the <em>softmax</em>, which normalizes vectors into probabilities. The output of the model, the embedding, may undergo an additional linear layer to transform it to a vector of size <span class="math inline">1‚ÄÖ√ó‚ÄÖ<em>N</em></span>, termed <em>logits</em>, where <span class="math inline"><em>N</em></span> is the number of classes. The logits, here denoted as <span class="math inline"><strong>v</strong></span>, are the input to the softmax operation defined as follows: <br /><span class="math display">$$\label{eq:softmax}
    \text{softmax}(v_i)=\frac{\mathrm{e}^{v_i}}{\sum_{j=1}^{N}\mathrm{e}^{v_j}}, ~~~~~ i\in[1,...,N].$$</span><br /></p>
<h1 id="sec:LF">Loss functions</h1>
<p>Defining the loss function of the model, denoted as <span class="math inline">‚Ñí</span>, is critical and usually chosen based on the characteristics of the dataset and the task at hand. Though datasets can vary, tasks performed by NN models can be divided into two coarse groups: (1) regression tasks and (2) classification tasks.</p>
<p>A <span><em>regression</em></span> problem aims at approximating a mapping function from input variables to a continuous output variable(s). For NN tasks, the output of the network should predict a continues value of interest. Common NN regression problems include image denoising¬†<span class="citation"></span>, deblurring¬†<span class="citation"></span>, inpainting¬†<span class="citation"></span> and more. In these tasks, it is common to use the Mean Squared Error (MSE), Structural SIMilarity (SSIM) or <span class="math inline">‚Ñì<sub>1</sub></span> loss as the loss function. The MSE (<span class="math inline">‚Ñì<sub>2</sub></span> error) imposes a larger penalty for larger errors, compared to the <span class="math inline">‚Ñì<sub>1</sub></span> error which is more robust to outliers in the data. The SSIM, and its multiscale version <span class="citation"></span>, help improving the perceptual quality.</p>
<p>In the <span><em>classification</em></span> task, the goal is to identify the correct class of a given sample from pre-defined <span class="math inline"><em>N</em></span> classes. A common loss function for such tasks is the <em>cross-entropy</em> loss. It is implemented based on a normalized vector of probabilities corresponding to a list of potential outcomes. This normalized vector is calculated by the softmax non-linear function (Eq.¬†). The cross-entropy loss is defined as: <br /><span class="math display">$$\label{eq:cross-entropy}
\mathcal{L}_{CE}=-\sum_{i=1}^{N}y_i\log(p_i),$$</span><br /> where <span class="math inline"><em>y</em><sub><em>i</em></sub></span> is the ground-truth probability (the label) of the input to belong to class <span class="math inline"><em>i</em></span> and <span class="math inline"><em>p</em><sub><em>i</em></sub></span> is the model prediction score for this class. The label is usually binary, i.e., it contains <span class="math inline">1</span> in a single index (corresponding to the true class). This type of representation is known as <em>one-hot encoding</em>. The class is predicted in the network by selecting the largest probability and the log-loss is used to increase this probability.</p>
<p>Notice that a network may provide multiple outputs per input data-point. For example, in the problem of image semantic segmentation, the network predicts a class for each pixel in the image. In the task of object detection, the network outputs a list of objects, where each is defined by a bounding box (found using a regression loss) and a class (found using a classification loss). Section¬†[subsec:detection_segmentation] details these different tasks. Since in some problems, the labelled data are imbalanced, one may use weighted softmax (that weigh less frequent classes) or the focal loss¬†<span class="citation"></span>.</p>
<h2 id="metric-learning">Metric Learning</h2>
<p>An interesting property of the log-loss function used for classification is that it implicitly cluster classes in the network embedding space during training. However, for a clustering task, these vanilla distance criteria often produce unsatisfactory performance as different class clusters can be positioned closely in the embedding space and may cause miss-classification for samples that do not reside in the specific training set distribution.</p>
<p>Therefore, different metric learning techniques have been developed to produce an embedding space that brings closer intra-class samples and increases inter-class distances. This results in better accuracy and robustness of the network. It allows the network to be able to distinguish between two data samples if they are from the same class or not, just by comparing their embeddings, even if their classes have not been present at training time.</p>
<p>Metric learning is very useful for tasks such as face recognition and identification, where the number of subjects to be tested are not known at training time and new identities that were not present during training should also be identified/recognized (e.g., given two images the network should decide whether these correspond to the same or different persons).</p>
<p>An example for a popular metric loss is the <em>triplet loss</em>¬†<span class="citation"></span>. It enforces a margin between instances of the same class and other classes in the embedding feature space. This approach increases performance accuracy and robustness due to the large separation between class clusters in the embedding space. The triplet loss can be used in various tasks, namely detection, classification, recognition and other tasks of unknown number of classes.</p>
<p>In this approach, three instances are used in each training step <span class="math inline"><em>i</em></span>: an anchor <span class="math inline"><strong>x</strong><sub><em>i</em></sub><sup><em>a</em></sup></span>, another instance <span class="math inline"><strong>x</strong><sub><em>i</em></sub><sup><em>p</em></sup></span> from the same class of the anchor (positive sample), and a sample <span class="math inline"><strong>x</strong><sub><em>i</em></sub><sup><em>n</em></sup></span> from a different class (negative class). They are required to obey the following inequality: <br /><span class="math display">‚Äñ<em>Œ¶</em>(<strong>x</strong><sub><em>i</em></sub><sup><em>a</em></sup>)‚àí<em>Œ¶</em>(<strong>x</strong><sub><em>i</em></sub><sup><em>p</em></sup>)‚Äñ<sub>2</sub><sup>2</sup>‚ÄÖ+‚ÄÖ<em>Œ±</em>‚ÄÑ&lt;‚ÄÑ‚Äñ<em>Œ¶</em>(<strong>x</strong><sub><em>i</em></sub><sup><em>a</em></sup>)‚àí<em>Œ¶</em>(<strong>x</strong><sub><em>i</em></sub><sup><em>n</em></sup>)‚Äñ<sub>2</sub><sup>2</sup>,</span><br /> where <span class="math inline"><em>Œ±</em>‚ÄÑ&lt;‚ÄÑ0</span> enforces the wanted margin from other classes. Thus, the triplet loss is defined by: <br /><span class="math display">‚Ñí‚ÄÑ=‚ÄÑ‚àë<sub><em>i</em></sub>‚Äñ<em>Œ¶</em>(<strong>x</strong><sub><em>i</em></sub><sup><em>a</em></sup>)‚àí<em>Œ¶</em>(<strong>x</strong><sub><em>i</em></sub><sup><em>p</em></sup>)‚Äñ<sub>2</sub><sup>2</sup>‚ÄÖ‚àí‚ÄÖ‚Äñ<em>Œ¶</em>(<strong>x</strong><sub><em>i</em></sub><sup><em>a</em></sup>)‚àí<em>Œ¶</em>(<strong>x</strong><sub><em>i</em></sub><sup><em>n</em></sup>)‚Äñ<sub>2</sub><sup>2</sup>‚ÄÖ+‚ÄÖ<em>Œ±</em>.</span><br /></p>
<p>Fig.¬†[fig:triplet] presents a schematic illustration of the triplet loss influence on samples in the embedding space. This illustration also exhibits a specific triplet example, where the positive examples are relatively far from the anchor while negative examples are relatively near the anchor. Finding such examples that violate the triplet condition is desirable during training. They may be found by on-line or off-line searches known as <em>hard negative mining</em>. A preprocessing of the instances in the embedding space is performed to find violating examples for training the network.</p>
<p>Finding the ‚Äúbest‚Äù instances for training can, evidently, aid in achieving improved convergence. However, searching for them is often time consuming and therefore alternative techniques are being explored.</p>
<div class="figure">
<img src="pics/triplet.png" alt="Triplet loss: minimizes the distance between two similar class examples (anchor and positive), and maximizes the distance between two different class examples (anchor and negative)." style="width:60.0%" />
<p class="caption">Triplet loss: minimizes the distance between two similar class examples (anchor and positive), and maximizes the distance between two different class examples (anchor and negative).<span data-label="fig:triplet"></span></p>
</div>
<p>An intriguing metric learning approach relies on ‚Äôclassification‚Äô-type loss functions, where the network is trained given a fixed number of classes. Yet, these losses are designed to create good embedding space that creates margin between classes, which in turn provides good prediction of similarity between two inputs. Popular examples include the Cos-loss <span class="citation"></span>, Arc-loss <span class="citation"></span> and SphereFace <span class="citation"></span>.</p>
<h1 id="sec:training">Neural network training</h1>
<p>Given a loss function, the weights of the neural network are updated to minimize it for a given training set. The training process of a neural network requires a large database due to the nature of the network (structure and amount of parameters) and GPUs for efficient training implementation.</p>
<p>In general, training methods can be divided into supervised and unsupervised training. The former consists of labeled data that are usually very expensive and time consuming to obtain. Whereas the latter is the more common case and does not assume known ground-truth labels. However, supervised training usually achieves significantly better network performance compared to the unsupervised case. Therefore, a lot of resources are invested in labeling datasets for training. Thus, we focus here mainly on the supervised setting.</p>
<p>In neural networks, regardless of the model task, all training phases have the same goal: to minimize a pre-defined error function, also denoted as the loss/cost function. This is done in two stages: (a) a feed-forward pass of the input data through all the network layers, calculating the error using the predicted outputs and their ground-truth labels (if available); followed by (b) backpropogation of the errors through the network to update their weights, from the last layer to the first. This process is performed continuously to find the optimized values for the weights of the network.</p>
<p>The backpropagation algorithm provides the gradients of the error with respect to the network weights. These gradients are used to update the weights of the network. Calculating them based on the whole input data is computationally demanding and therefore, the common practice is to use subsets of the training set, termed <em>mini-batches</em>, and cycle over the entire training set multiple times. Each cycle of training over the whole dataset is termed an <em>epoch</em> and in every cycle the data samples are used in a random order to avoid biases. The training process ends when convergence in the loss function is obtained. Since most NN problems are not convex, an optimal solution is not assured. We turn now to describe in more details the training process using backpropagation.</p>
<h2 id="subsec:backprop">Backpropogation</h2>
<p><span>r</span><span>5.5cm</span> <img src="pics/example.png" alt="image" style="width:20.0%" /></p>
<p>The backpropagation process is performed to update all the parameters of the model, with the goal of decreasing the loss function value. The process starts with a feed-forward pass of input data, <span class="math inline"><strong>x</strong></span>, through all the network layers. After which the loss function value is calculated and denoted as <span class="math inline">${\mathcal{L}}(\mathbf{x},{\bf W})$</span>, where <span class="math inline">${\bf W}$</span> are the model parameters (including the model weights and biases, for formulation convenience). Then the backpropagation is initiated by computing the value of:¬†<span class="math inline">$\frac{\partial {\mathcal{L}}}{\partial {\bf W}}$</span>, followed by the update of the network weights. All the weights are updated recursively by calculating the gradients of every layer, from the final one to the input layer, using the chain rule.</p>
<p>Denote the output of layer <span class="math inline"><em>l</em></span> as <span class="math inline">${\bf z}^{(l)}$</span>. Following the chain rule, the gradients of a given layer <span class="math inline"><em>l</em></span> with parameters <span class="math inline">${\bf W}^{(l)}$</span> with respect to its input <span class="math inline">${\bf z}^{(l)}$</span> are: <br /><span class="math display">$$\frac{\partial {\mathcal{L}}}{\partial {\bf z}^{(l-1)}}=\frac{\partial {\mathcal{L}}}{\partial {\bf z}^{(l)}}\cdot\frac{\partial {\bf z}^{(l)}({\bf W}^{(l)},{\bf z}^{(l-1)})}{\partial {\bf z}^{(l-1)}},$$</span><br /> and the gradients with respect to the parameters are: <br /><span class="math display">$$\frac{\partial {\mathcal{L}}}{\partial {\bf W}^{(l)}}=\frac{\partial {\mathcal{L}}}{\partial {\bf z}^{(l)}}\cdot\frac{\partial {\bf z}^{(l)}({\bf W}^{(l)},{\bf z}^{(l-1)})}{\partial {\bf W}^{(l)}}.$$</span><br /> These two formulas of the backpropagation algorithm dictate the gradients calculation with respect to the parameters for each layer in the network and, therefore, the optimization can be performed using gradient-based optimizers (see Section¬†[sec:optimizers] for more details).</p>
<p>To demonstrate the use of the backpropagation technique for the calculation of the network gradients, we turn to consider an example of a simple classification model with two-layers: a fully-connected layer with a ReLU activation function followed by another fully-connected layer with softmax function and log-loss. See Fig.¬†[fig:example] for the model illustration.</p>
<p>Denote by <span class="math inline">${\bf z}^{(3)}$</span> the output of the softmax layer and assume that the input <span class="math inline"><strong>x</strong></span> belongs to class <span class="math inline"><em>k</em></span> (using one-hot encoding <span class="math inline"><em>y</em><sub><em>k</em></sub>‚ÄÑ=‚ÄÑ1</span>). The log-loss in this case is: <br /><span class="math display">$${\mathcal{L}}=-\sum_i\log\big(z_i^{(3)}\big)y_i=-\log\Bigg(\frac{\exp\big(z^{(2)}_k\big)}{\sum_i\exp\big(z^{(2)}_i\big)}\Bigg)=-z^{(2)}_k+\log\Big(\sum_j \exp{z^{(2)}_j}\Big).$$</span><br /> For all <span class="math inline"><em>i</em>‚ÄÑ‚â†‚ÄÑ<em>k</em></span>, the gradient of the error with respect to the softmax input <span class="math inline"><em>z</em><sub><em>i</em></sub><sup>(2)</sup></span> is <br /><span class="math display">$$\frac{\partial {\mathcal{L}}}{\partial z^{(2)}_i}=\frac{\exp\big({z^{(2)}_i}\big)}{\sum_j\exp\big(z^{(2)}_j\big)}\equiv g_i.$$</span><br /> Notice that this implies that we need to decrease the value of <span class="math inline"><em>z</em><sub><em>i</em></sub><sup>(2)</sup></span> (the <span class="math inline"><em>i</em><sup>th</sup></span>-logit) proportionally to the probability the network provides to it. While for the correct label, <span class="math inline"><em>i</em>‚ÄÑ=‚ÄÑ<em>k</em></span>, the derivative is: <br /><span class="math display">$$\frac{\partial {\mathcal{L}}}{\partial z^{(2)}_k}=-1+\frac{\exp\big({z^{(2)}_k}\big)}{\sum_j\exp\big(z^{(2)}_j\big)}= g_k-1,$$</span><br /> which implies that the value of the logit element associated with the true label should be increased proportionally to the mistake the network is currently doing in the prediction.</p>
<p>The output <span class="math inline">${\bf z}^{(2)}$</span> is a product of a fully-connect layer. Therefore, it can be formulated as follows: <br /><span class="math display">$${\bf z}^{(2)}={\bf W}^{(2)}\tilde{{\bf z}}^{(1)},$$</span><br /> where <span class="math inline">$\tilde{{\bf z}}^{(1)}$</span> is the output of the ReLu function. Following the backpropagation rules we get that for this layer, the derivative with respect to its input is: <br /><span class="math display">$$\label{eq:backprop_fc2}
    \frac{\partial {\mathcal{L}}}{\partial \tilde{{\bf z}}^{(1)}}=\frac{\partial {\mathcal{L}}}{\partial {\bf z}^{(2)}}\cdot\frac{\partial {\bf z}^{(2)}({\bf W}^{(2)},\tilde{{\bf z}}^{(1)})}{\partial \tilde{{\bf z}}^{(1)}}=\frac{\partial {\mathcal{L}}}{\partial {\bf z}^{(2)}}\cdot{\bf W}^{(2)},$$</span><br /> whereas, the derivative with respect to its parameters is: <br /><span class="math display">$$\frac{\partial {\mathcal{L}}}{\partial {\bf W}^{(2)}}=\frac{\partial {\mathcal{L}}}{\partial {\bf z}^{(2)}}\cdot\frac{\partial {\bf z}^{(2)}({\bf W}^{(2)},\tilde{{\bf z}}^{(1)})}{\partial {\bf W}^{(1)}}=\frac{\partial {\mathcal{L}}}{\partial {\bf z}^{(2)}}\cdot \tilde{{\bf z}}^{(1)}.$$</span><br /> The ReLU operation has no weight to update, but affects the gradients. The derivative of this stage follows: <br /><span class="math display">$$\frac{\partial {\mathcal{L}}}{\partial {\bf z}^{(1)}}=\frac{\partial {\mathcal{L}}}{\partial \tilde{{\bf z}}^{(1)}}\cdot\frac{\partial \tilde{{\bf z}}^{(1)}({\bf W}^{(1)},I)}{\partial {\bf z}^{(1)}}=\begin{cases}
0, &amp;\text{if } {\bf z}^{(1)}&lt;0\\
\frac{\partial {\mathcal{L}}}{\partial \tilde{{\bf z}}^{(1)}}, &amp;\text{otherwise}.
\end{cases}$$</span><br /> The final derivative with respect to the input <span class="math inline">‚àÇ‚Ñí/‚àÇ<strong>x</strong></span> is calculated similar to Eq.¬†.</p>
<h2 id="training-considerations">Training considerations</h2>
<p>There are several considerations that should be addressed when training a NN. The most infamous is the <em>overfitting</em>, i.e., when the model too closely fits to the training dataset but does not generalize well to the test set. When this occurs, high training data precision is achieved, while the precision on the test data (not used during training) is low <span class="citation"></span>. For this purpose, various regularization techniques have been proposed. We discuss some of them in Section¬†[sec:regularizations].</p>
<p>A second consideration is the vanishing/exploding gradients occurring during training. Vanishing gradients are a result of multiplications with values smaller than one during their calculation in the backpropagation recursion. This can be resolved using activation functions and batch normalization detailed in Section¬†[sec:regularizations]. On the other hand, the gradients might also explode due to derivatives that are significantly larger than one in the backpropogation calculation. This makes the training unstable and may imply the need for re-designing the model (e.g., replace a vanilla RNN with a gated architecture such as LSTMs) or the use of gradient clipping¬†<span class="citation"></span>.</p>
<p>Another important issue is the requirement that the training dataset must represent the true distribution of the task at hand. This usually enforces very large annotated datasets, which necessitate significant funding and manpower to obtain. In this case, considerable efforts must be invested to train the network using these large datasets, commonly with multiple GPUs for several days¬†<span class="citation"></span>. One may use techniques such as domain adaptation¬†<span class="citation"></span> or transfer learning¬†<span class="citation"></span> to use already existing networks or large datasets for new tasks.</p>
<h1 id="sec:optimizers">Training optimizers</h1>
<p>Training neural networks is done by applying an optimizer to reach an optimal solution for the defined loss function. Its goal is to find the parameters of the model, e.g., weights and biases, which achieve minimum error for the training set samples: <span class="math inline">(<strong>x</strong><sub><em>i</em></sub>,‚ÄÜ<em>y</em><sub><em>i</em></sub>)</span>, where <span class="math inline"><em>y</em><sub><em>i</em></sub></span> is the label for the instance <span class="math inline"><strong>x</strong><sub><em>i</em></sub></span>. For a loss function <span class="math inline">‚Ñí(‚ãÖ)</span>, the objective reads as: <br /><span class="math display">‚àë<sub><em>i</em></sub>‚Ñí(<em>Œ¶</em>(<strong>x</strong><sub><em>i</em></sub>,‚ÄÜ<strong>W</strong>),<em>y</em><sub><em>i</em></sub>),</span><br /> for ease of notation, all model parameters are denoted as <span class="math inline"><strong>W</strong></span>. A variety of optimizers have been proposed and implemented for minimizing Eq.¬†[eq:training_error]. Yet, due to the size of the network and training dataset, mainly first-order methods are being considered, i.e. strategies that rely only on the gradients (and not on second-order derivatives such as the Hessian).</p>
<p>Several gradient based optimizers are commonly used for updating the parameters of the model. These NN parameters are updated in the opposite direction of the objective function‚Äôs gradient, <span class="math inline"><em>g</em><sub>{GD,‚ÄÜùíØ(<em>t</em>)}</sub></span>, where <span class="math inline">ùíØ(<em>t</em>)</span> is a randomly chosen subgroup of size <span class="math inline"><em>n</em>‚Ä≤&lt;<em>n</em></span> training samples used in iteration <span class="math inline"><em>t</em></span> (<span class="math inline"><em>n</em></span> is the size of the training dataset). Namely, at iteration <span class="math inline"><em>t</em></span> the weights are calculated as <br /><span class="math display"><strong>W</strong>(<em>t</em>)=<strong>W</strong>(<em>t</em>‚ÄÖ‚àí‚ÄÖ1)‚àí<em>Œ∑</em>‚ÄÖ‚ãÖ‚ÄÖ<em>g</em><sub>{GD,‚ÄÜùíØ(<em>t</em>)}</sub>,</span><br /> where <span class="math inline"><em>Œ∑</em></span> is the learning rate that determines the size of the steps taken to reach the (local) minimum and the gradient step, <span class="math inline"><em>g</em><sub>{GD,‚ÄÜùíØ(<em>t</em>)}</sub></span> is computed using the samples in <span class="math inline">ùíØ(<em>t</em>)</span> as <br /><span class="math display">$$\label{eq:GD}
g_{\text{\{GD},\mathcal{T}(t)\}} = \frac{1}{n'}\sum_{i\in \mathcal{T}(t)}\nabla _{W}\mathcal{L}(\mathbf{W}(t);\mathbf{x}_i;y_i),$$</span><br /> where the pair <span class="math inline">(<strong>x</strong><sub><em>i</em></sub>,‚ÄÜ<em>y</em><sub><em>i</em></sub>)</span> is a training example and its corresponding label in the training set, and <span class="math inline">‚Ñí</span> is the loss function. However, needless to say that calculating the gradient on the whole dataset is computationally demanding. To this end, Stochastic Gradient Descent (SGD) is more popular, since it calculates the gradient in Eq. for only one randomly chosen example from the data, i.e., <span class="math inline"><em>n</em>‚Ä≤=1</span>.</p>
<p>Since the update by SGD depends on a different sample at each iteration, it has a high variance that causes the loss value to fluctuate. While this behavior may enable it to jump to a new and potentially better local minima, it might ultimately complicates convergence, as SGD may keep overshooting. To improve convergence and exploit parallel computing power, mini-batch SGD is proposed in which the gradient in Eq.¬† is calculated with <span class="math inline"><em>n</em>‚Ä≤&gt;1</span> (but not all the data).</p>
<p>An acceleration in convergence may be obtained by using the history of the last gradient steps, in order to stabilize the optimization. One such approach uses adaptive momentum instead of a fixed step size. This is calculated based on exponential smoothing on the gradients, i.e: <br /><span class="math display">$$\begin{aligned}
    M(t)&amp;=\beta\cdot M(t-1)+(1-\beta)\cdot g_{\{\text{SGD},\mathcal{T}(t)\}},\\
    \mathbf{W}(t)&amp;=\mathbf{W}(t-1)-\eta M(t),
\end{aligned}$$</span><br /> where <span class="math inline"><em>M</em>(<em>t</em>)</span> approximates the <span class="math inline">1<sup>st</sup></span> moment of <span class="math inline"><em>g</em><sub>{SGD,‚ÄÜùíØ(<em>t</em>)}</sub></span>. A typical value for the constant is , which implies taking into account the last <span class="math inline">10</span> gradient steps in the momentum variable <span class="math inline"><em>M</em>(<em>t</em>)</span>¬†<span class="citation"></span>. A well-known variant of Momentum proposed by Nestrov <em>et al.</em>¬†<span class="citation"></span> is the Nestrov Accelerated Gradient (NAG). It is similar to Momentum but calculates the gradient step as if the network weights have been already updated with the current Momentum direction.</p>
<p>Another popular technique is the Adaptive Moment Estimation (ADAM)¬†<span class="citation"></span>, which also computes adaptive learning rates. In addition to storing an exponentially decaying average of past squared gradients, <span class="math inline"><em>V</em>(<em>t</em>)</span>, ADAM also keeps an exponentially decaying average of past gradients, <span class="math inline"><em>M</em>(<em>t</em>)</span>, in the following way: <br /><span class="math display">$$\begin{aligned}
M(t)&amp;=\beta_1M(t-1)+(1-\beta_1)g_t, \\
V(t)&amp;=\beta_2V(t-1)+(1-\beta_2)g_t^2,
\end{aligned}$$</span><br /> where <span class="math inline"><em>g</em><sub><em>t</em></sub></span> is the gradient of the current batch, <span class="math inline"><em>Œ≤</em><sub>1</sub></span> and <span class="math inline"><em>Œ≤</em><sub>2</sub></span> are ADAM‚Äôs hyperparameters, usually set to 0.9 and 0.999 respectively, and <span class="math inline"><em>M</em>(<em>t</em>)</span> and <span class="math inline"><em>V</em>(<em>t</em>)</span> are estimates of the first moment (the mean) and the second moment (the uncentered variance) of the gradients respectively. Hence the name of the method - Adaptive Moment Estimation. As <span class="math inline"><em>M</em>(<em>t</em>)</span> and <span class="math inline"><em>V</em>(<em>t</em>)</span> are initialized as vectors of 0‚Äôs, the authors of ADAM observe that they are biased towards zero, especially during the initial time steps. To counteract these biases, a bias-corrected first and second moment are used: and . Therefore, the ADAM update rule is as follows: <br /><span class="math display">$$\mathbf{W}(t+1)=\mathbf{W}(t)-\frac{\eta}{\sqrt{\hat{V}(t)+\epsilon}}\hat{M}(t).$$</span><br /> ADAM has two popular extensions: AdamW by Loshchilov <em>et al.</em>¬†<span class="citation"></span> and AMSGrad by Redddi <em>et al.</em>¬†<span class="citation"></span>. There are several additional common optimizers that have adaptive momentum, such as AdaGrad <span class="citation"></span>, AdaDelta <span class="citation"></span> or RMSprop <span class="citation"></span>. It must be noted that since the NN optimization is non-convex, the minimal error point reached by each optimizer is rarely the same. Thus, speedy convergence is not always favored. In particular, it has been observed that Momentum leads to better generalization than ADAM, which usually converges faster¬†<span class="citation"></span>. Thus, the common practice is to make the development with ADAM and then make the final training with Momentum.</p>
<p>[b]<span>0.495</span> <img src="pics/cat.png" title="fig:" alt="Different image augmentations." /> [fig:org]</p>
<p>[b]<span>0.495</span> <img src="pics/cat_flip.png" title="fig:" alt="Different image augmentations." /> [fig:flip]</p>
<p><br />
</p>
<p>[b]<span>0.495</span></p>
<table>
<tbody>
<tr class="odd">
<td align="center"><img src="pics/cat_crop.png" title="fig:" alt="Different image augmentations." /></td>
<td align="center"><img src="pics/cat_crop.png" title="fig:" alt="Different image augmentations." /></td>
</tr>
</tbody>
</table>
<p>[fig:crop]</p>
<p>[b]<span>0.495</span> <img src="pics/cat_noised.png" title="fig:" alt="Different image augmentations." /> [fig:noise]</p>
<h1 id="sec:regularizations">Training regularizations</h1>
<p>One of the great advantageous of NN is their ability to generalize, i.e., correctly predict unseen data <span class="citation"></span>. This must be ensured during the training process and is accomplished by several regularization methods, detailed here. The most common are weight decay¬†<span class="citation"></span>, dropout¬†<span class="citation"></span>, batch normalization¬†<span class="citation"></span> and the use of data augmentation¬†<span class="citation"></span>.</p>
<p><em>Weight decay</em> is a basic tool to limit the growth of the weights by adding a regularization term to the cost function for large weights, which is the sum of squares of all the weights, i.e., <span class="math inline">‚àë<sub><em>i</em></sub>|<em>W</em><sub><em>i</em></sub>|<sup>2</sup></span>.</p>
<p>The key idea in <em>dropout</em> is to randomly drop units (along with their connections) from the neural network during training and thus prevent units from co-adapting too much. The percentage of dropped units is critical since a large amount will result in poor learning. Common values are <span class="math inline">20%‚àí50%</span> dropped units.</p>
<p><em>Batch normalization</em> is a mean to deal with changes in the distribution of the model‚Äôs parameters during training. The layers need to adapt to these (often noisy) changes between instances during training. Batch normalization causes the features of each training batch to have a mean of 0 and a variance of 1 in the layer it is being applied. To normalize a value across a batch, i.e. to batch normalize the value, the batch mean, <span class="math inline"><em>Œº</em><sub><em>B</em></sub></span>, is subtracted and the result is divided by the batch standard deviation, <span class="math inline">$\sqrt{\sigma _B^2+\epsilon}$</span>. Note that a small constant <span class="math inline"><em>œµ</em></span> is added to the variance in order to avoid dividing by zero. The batch normalizing transform of a given input, <span class="math inline"><strong>x</strong></span>, is: <br /><span class="math display">$$\text{BN}(\mathbf{x})=\gamma \Bigg( \frac{\mathbf{x}-\mu_B}{\sqrt{\sigma _B^2+\epsilon}}\Bigg)+\beta.$$</span><br /> Notice the (learnable) scale and bias parameters <span class="math inline"><em>Œ≥</em></span> and <span class="math inline"><em>Œ≤</em></span>, which provides the NN with freedom to deviate from the zero mean and unit variance. BN is less effective when used with small batch sizes since in this case the statistics calculated per each is less accurate. Thus, techniques such as group normalization¬†<span class="citation"></span> or Filter Response Normalization (FRN)¬†<span class="citation"></span> have been proposed.</p>
<p><em>Data augmentation</em> is a very common strategy used during training to artificially ‚Äúincrease‚Äù the size of the training data and make the network robust to transformations that do not change the input label. For example, in the task of classification a shifted cat is still a cat; see Fig. [fig:augmentations] for more similar augmentation. In the task of denoising, flipped noisy input should result in a flipped clean output. Thus, during training the network is trained also with the transformed data to improve its performance.</p>
<p>Common augmentations are randomly flipping, rotating, scaling, cropping, translating, or adding noise to the data. Other more sophisticated techniques that lead to a significant improvement in network performance include mixup <span class="citation"></span>, cutout¬†<span class="citation"></span> and augmentations that are learned automatically <span class="citation"></span>.</p>
<h1 id="sec:architectures">Advanced NN architectures</h1>
<p>The basic building blocks, which compose the NN model architecture, are used in frequently innovative structures. In this section, such known architectures with state-of-the-art performance are presented, divided by tasks and data types: detection and segmentation tasks are described in Section¬†[subsec:detection_segmentation], sequential data handling is elaborated in Section¬†[subsec:sequential] and processing data on irregular grids is presented in Section¬†[subsec:irregular_grids]. Clearly, there are many other use-cases and architectures, which are not mentioned here.</p>
<h2 id="subsec:detection_segmentation">Deep learning for detection and segmentation</h2>
<p>Many research works focus on detecting multiple objects in a scene, due to its numerous applications. This problem can be divided into four sub-tasks as follows, where we refer here to image datasets although the same concept can be applied to different domains as well.</p>
<ol>
<li><p><em>Classification and localization</em>: The main object in the image is detected and then localized by a surrounding bounding box and classified from a pre-known set.</p></li>
<li><p><em>Object detection</em>: Detection of all objects in a scene that belong to a pre-known set and then classifying and providing a bounding box for each of them.</p></li>
<li><p><em>Semantic segmentation</em>: Partitioning the image into coherent parts by assigning each pixel in the image with its own classification label (associated with the object the pixel belongs to). For example, having a pixel-wise differentiation between animals, sky and background (generic class for all object that no class is assigned to) in an image.</p></li>
<li><p><em>Instance segmentation</em>: Multiple objects segmentation and classification from a pre-known set (similar to object detection but for each object all its pixels are identified instead of providing a bounding box for it).</p></li>
</ol>
<p>Today, state-of-the-art object detection performance is achieved with architectures such as Faster-RCNN¬†<span class="citation"></span>, You Only Look Once (YOLO)¬†<span class="citation"></span>, Single Shot Detector (SSD)¬†<span class="citation"></span> and Fully Convolutional One-Stage Object Detection (FCOS)¬†<span class="citation"></span>. The object detection models provide a list of detected bounding boxes with the class of each of them.</p>
<p>Segmentation tasks are mostly implemented using fully convolutional network. Known segmentation models include UNet¬†<span class="citation"></span>, Mask-RCNN¬†<span class="citation"></span> and Deeplab¬†<span class="citation"></span>. These architecture have the same input/output spatial size since the output represents the segmentation map of the input image.</p>
<p>Both object detection and segmentation tasks are analyzed via the Intersection over Union (IoU) metric. The IoU is defined as the ratio between the intersection area of the object‚Äôs ground-truth pixels, <span class="math inline"><em>B</em><sub><em>g</em></sub></span>, with the corresponding predicted pixels, <span class="math inline"><em>B</em><sub><em>p</em></sub></span>, and the union of these group of pixels. The IoU is formulated as: <br /><span class="math display">$$\text{IoU}=\frac{\text{Area}\{B_g\cap B_p\}}{\text{Area}\{B_g\cup B_p\}}.$$</span><br /> As this measure evaluate only the quality of the bounding box, a mean Average Precision (mAP) is commonly used to evaluate the models performance. The mAP is defined as the ratio of the correctly detected (or segmented) objects, where an object is considered to be detected correctly if there is a bounding box for it with the correct class and a IoU greater than 0.5 (or another specified constant).</p>
<p>Another common evaluation metric is the F1 score, which is the harmonic average of the precision and the recall values. See Eq.¬† below. They are calculated using the following definitions that are presented for the case of semantic segmentation:</p>
<ul>
<li><p>True Positive (TP): the predicted class of a pixel matches it ground-truth label.</p></li>
<li><p>False Positive (FP): the predicted pixel of an object was falsely determined.</p></li>
<li><p>False Negative (FN): a ground-truth pixel of an object was not predicted.</p></li>
</ul>
<p>Now that they are defined, the <em>precision</em>, <em>recall</em> and F1 are given by: <br /><span class="math display">$$\text{precision}=\frac{\text{TP}}{\text{TP}+\text{FP}},\hspace{10pt}
    \text{recall}=\frac{\text{TP}}{\text{TP}+\text{FN}}$$</span><br /> <br /><span class="math display">$$\label{eq:F1_score}
    \text{F1}=2\cdot\frac{\text{precision}\cdot\text{recall}}{\text{precision}+\text{recall}}.$$</span><br /></p>
<h2 id="subsec:sequential">Deep learning on sequential data</h2>
<p>Sequential data are composed of time-sensitive signals such as the output of different sensors, audio recordings, NLP sentences or any signal that its order is of importance. Therefore, this data must be processed accordingly.</p>
<p>Initially, sequential data was processed with Recurrent NN (RNN)¬†<span class="citation"></span> that has recurrent (feedback) connections, where outputs of the network at a given time-step serve as input to the model (in addition to the input data) at the next time-step. This introduces the time dependent feature of the NN. A RNN is illustrated in Fig.¬†[fig:RNN].</p>
<p>However, it was quickly realized that during training, vanilla RNNs suffer from vanishing/exploding gradients. This phenomena, originated from the use of finite-precision back-propagation process, limits the size of the sequence.</p>
<p>To this end, a corner stone block is used: the Long-Short-Term-Memory (LSTM¬†<span class="citation"></span>). Mostly used for NLP tasks, the LSTM is a RNN block with gates. During training, these gates learn which part of the sentence to forget or to memorize. The gating allow some of the gradients to backpropagate unchanged, which aids the vanishing gradient symptom. Notice that RNNs (and LSTMs) can process a sentence in a bi-directional mode, i.e., process a sentence in two directions, from the beginning to the end and vice verse. This mechanism allows a better grasp of the input context by the network. Examples for popular research tasks in NLP data include question answering¬†<span class="citation"></span>, translation¬†<span class="citation"></span> and text generation¬†<span class="citation"></span>.</p>
<p><span><strong>Sentences processing.</strong></span> An important issue in NLP is representing words in preparation to serve as network input. The use of straight forward indices is not effective since there are thousands of words in a language. Therefore, it is common to process text data via <em>word embedding</em>, which is a vector representation of each word in some fixed dimension. This method enables to encapsulate relationships between words.</p>
<p>A classic methodology to calculate the word embedding is <em>Word2Vec</em>¬†<span class="citation"></span>, in which these vector representations are calculated using a NN model that learn their context. More advanced options for creating efficient word representations include BERT¬† <span class="citation"></span>, ELMO¬†<span class="citation"></span>, RoBERTa¬†<span class="citation"></span> and XLNet¬†<span class="citation"></span>.</p>
<p><span><strong>Audio processing.</strong></span> Audio recordings are used for multiple interesting tasks, such as speech to text, text to speech and speech processing. In the audio case, the common input to speech systems is the Mel Frequency Cepstral Coefficient (MFCC) or a Short Time Fourier Transform (STFT) image, as opposed to the audio raw-data. A milestone example for speech processing NN architecture is the <em>wavenet</em> <span class="citation"></span>. This architecture is an autoregressive model that synthesizes speech or audio signals. It is based on dilated convolutional layers that have large receptive fields, that allow efficient processing. Another prominent synthesis model for sequential data is the Tacotron¬†<span class="citation"></span>.</p>
<p><span><strong>The attention model.</strong></span> As mentioned in Section¬†[sec:basic_structure], one may use RNN for translation using the encoder decoder model, which encodes a source sentence into a vector, which is then decoded to a target language. Instead of relying on a compressed vector, which may lose information, the <em>attention models</em> learn where or what to focus on from the whole input sequence. Introduced in 2015¬†<span class="citation"></span>, attention models have shown superior performance over encoder-decoder architectures in tasks such as translation, text to speech and image captioning. Recently, it has been suggested to replace the recurrent network structure totally by the attention mechanism, which results with the <em>transformers network</em> models¬†<span class="citation"></span>.</p>
<h2 id="subsec:irregular_grids">Deep learning on irregular grids</h2>
<p>A wide variety of data acquisition mechanisms do not represent the data on a grid as is common with images data. A prominent example is 3D imaging (e.g. using LIDAR), where the input data are represented as points in a 3D space with or without color information. Processing such data is not trivial as standard network components, such as convolutions, assume a grid of the data. Therefore, they cannot be applied as is and custom operations are required. We focus our discussion here on the case of NN for 3D data.</p>
<p>Today, real-time processing of 3D scenes can be achieved with advanced NN models that are customized to these irregular grids. The different processing techniques for these irregular grid data can be divided by the type of representation used for the data:</p>
<ol>
<li><p><span><strong>Points processing.</strong></span> 3D data points are processed as points in space, i.e., a list of the point coordinates is given as the input to the NN. A popular network for this representation is <em>PointNet</em>¬†<span class="citation"></span>. It is the first to efficiently achieve satisfactory results directly on the point cloud. Yet, it is limited by the number of points that can be analyzed, computational time and performance. Some more recent models that improves its performance include PointNet++¬†<span class="citation"></span>, PointCNN¬†<span class="citation"></span>, DGCNN <span class="citation"></span>. Strategies to improve its efficiency have been proposed in learning to sample¬†<span class="citation"></span> and RandLA-Net¬†<span class="citation"></span>.</p></li>
<li><p><span><strong>Multi-view 2D projections.</strong></span> 3D data points are projected (from various angles) to the 2D domain so that known 2D processing techniques can be used¬†<span class="citation"></span>.</p></li>
<li><p><span><strong>Volumetric (voxels).</strong></span> 3D data points are represented in a grid-based <em>voxel</em> representation. This is analogous to a 2D representation and is therefore advantageous. However, it is computationally exhaustive¬†<span class="citation"></span> and losses resolution.</p></li>
<li><p><span><strong>Meshes.</strong></span> Mesh represents the 3D domain via a graph that defines the connectivity between the different points. Yet, this graph has a special structure such that it creates the surface of the 3D shape (in the common case of triangular mesh, the shape surface is presented by a set of triangles connected to each other). In 2015 Masci <em>et al.</em>¬†<span class="citation"></span> have shown it is possible to learn features using DL on meshes. Since then, a significant advancement has been made in mesh processing¬†<span class="citation"></span>.</p></li>
<li><p><span><strong>Graphs.</strong></span> Graph representations are common for representing non-linear structured data. Some works have proposed efficient NN models for 3D data points on a grid-based graph structure¬†<span class="citation"></span>.</p></li>
</ol>
<h1 id="sec:summary">Summary</h1>
<p>This chapter provided a general survey of the basic concepts in neural networks. As this field is expanding very fast, the space is too short to describe all the developments in it, even though most of them are from the past eight years. Yet, we briefly mention here few important problems that are currently being studied.</p>
<ol>
<li><p><span><strong>Domain adaptation and transfer learning.</strong></span> As many applications necessitate data that is very difficult to obtain, some methods aim at training models based on scarce datasets. A popular methodology for dealing with insufficient annotated data is <em>domain adaptation</em>, in which a robust and high performance NN model, trained on a source distribution, is used to aid the training of a similar model (usually with the same goal, e.g., in classification the same classes are searched for) on data from a target distribution that are either unlabelled or small in number <span class="citation"></span>. An example is adapting a NN trained on simulation data to real-life data with the same labels¬†<span class="citation"></span>. On a similar note, <em>transfer learning</em>¬†<span class="citation"></span> can also be used in similar cases, where in addition to the difference in the data, the input and output tasks are not the same but only similar (in domain adaptation the task is the same and only the distributions are different). One such example, is using a network trained on natural images to classify medical data¬†<span class="citation"></span>.</p></li>
<li><p><span><strong>Few shot learning.</strong></span> A special case of learning with small datasets is <em>few-shot learning</em>¬†<span class="citation"></span>, where one is provided either with just semantic information of the target classes (zero-shot learning), only one labelled example per class (1-shot learning) or just few samples (general few-shot learning). Approaches developed for these problems have shown great success in many applications, such as image classification¬†<span class="citation"></span>, object detection¬†<span class="citation"></span> and segmentation¬†<span class="citation"></span>.</p></li>
<li><p><span><strong>On-line learning.</strong></span> Various deep learning challenges occur due to new distributions or class types introduced to the model during a continuous operation of the system (post-training), and now must be learnt by the model. The model can update its weights to incorporate these new data using <em>on-line learning</em> techniques. There is a need for special training in this case, as systems that just learn based on the new examples may suffer from a reduced performance on the original data. This phenomena is known as catastrophic forgetting¬†<span class="citation"></span>. Often, the model tends to forget the representation of part of the distribution it already learned and thus it develops a bias towards the new data. A specific example of on-line learning is <em>incremental learning</em>¬†<span class="citation"></span>, where the new data is of different classes than the original ones.</p></li>
<li><p><span><strong>AutoML.</strong></span> When approaching real-life problems, there is an inherent pipeline of tasks to be preformed before using DL tools, such as problem definition, preparing the data and processing it. Commonly, these tasks are preformed by specialists and require deep system understating. To this end, the <em>autoML</em> paradigm attempts to generalize this process by automatically learning and tuning the model used¬†<span class="citation"></span>.</p>
<p>A particular popular task in autoML is <em>Neural Architecture Search (NAS)</em>¬†<span class="citation"></span>. This is of interest since the NN architecture restricts its performance. However, searching for the optimal architecture for a specific task, and from a set of pre-defined operations, is computationally exhaustive when performed in a straight forward manner. Therefore, on-going research attempts to overcome this limitation. An example is the DARTS¬†<span class="citation"></span> strategy and its extensions¬†<span class="citation"></span> where the key contribution is finding, in a differentiable manner, the connections between network operations that form a NN architecture. This framework decreases the search time and improves the final accuracy.</p></li>
<li><p><span><strong>Reinforcement Learning.</strong></span> To date, the most effective training method for decision based actions, such as robot movement and video games, is <em>Reinforcement Learning</em> (RL)¬†<span class="citation"></span>. In RL, the model tries to maximize some pre-defined award score by learning which action to take, from a set of defined actions in specific scenarios.</p></li>
</ol>
<p>To summarize, being able to efficiently train deep neural networks has revolutionized almost every aspect of the modern day-to-day life. Examples span from bio-medical applications through computer graphics in movies and videos to international scale applications of big companies, such as Google, Amazon, Microsoft, Apple and Facebook. Evidently, this theory is drawing much attention and we believe there is still much to unravel, including exploring and understanding the NN‚Äôs potential abilities and limitations.</p>
<p>The next chapters detail Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), generative models and autoencoders. All are very important paradigms that are used in numerous applications.</p>
</body>
</html>
