{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTI-DISEASE DETECTION IN RETINAL IMAGING - an overview of this paper\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "***Author:*** Atanas Kuzmanov\n",
    "\n",
    "***Date:*** 2022-February-20\n",
    "\n",
    "*This is an article developed as a scientific notebook for an exam project assignment for a Deep Learning course from an Artificial Intelligence module.*\n",
    "\n",
    "*One of the aims of this article is to understand some Deep Learning (DL) basics, more specifically to understand Neural Networks (NNs) and how to improve them, so we can create models, train them, test them and extract predictions and information we might be interested in.*\n",
    "\n",
    "\n",
    "*This paper is a retelling and an overview of the original paper, where most of the contents deemed important have been unchanged:*\n",
    "\n",
    "\n",
    "**MULTI-DISEASE DETECTION IN RETINAL IMAGING**\n",
    "\n",
    "**BASED ON ENSEMBLING HETEROGENEOUS DEEP LEARNING MODELS**\n",
    "\n",
    "*Dominik Müller1, Iñaki Soto-Rey1,2 and Frank Kramer1*\n",
    "\n",
    "1 IT-Infrastructure for Translational Medical Research, University of\n",
    "Augsburg, Germany\n",
    "\n",
    "2 Medical Data Integration Center, University Hospital Augsburg, Germany\n",
    "\n",
    "Published: `[v1] Fri, 26 Mar 2021 18:02:17 UTC (757 KB)`\n",
    "\n",
    "`References:`\n",
    "\n",
    "\n",
    "Paper:\n",
    "\n",
    "- MULTI-DISEASE DETECTION IN RETINAL IMAGING BASED ON ENSEMBLING HETEROGENEOUS DEEP LEARNING MODELS\n",
    "[[Reference]](#MULTI-DISEASE-DETECTION-IN-RETINAL-IMAGING-PDF)\n",
    "\n",
    "- Multi-Disease Detection in Retinal Imaging - papers with code\n",
    "[[Reference]](#Multi-Disease-Detection-in-Retinal-Imaging---papers-with-code)\n",
    "\n",
    "Code:\n",
    "\n",
    "- Multi-Disease Detection in Retinal Imaging - GitHub\n",
    "[[Reference]](#Multi-Disease-Detection-in-Retinal-Imaging---GitHub)\n",
    "\n",
    "- AUCMEDI - A Framework for Automated Classification of Medical Images\n",
    "[[Reference]](#AUCMEDI---A-Framework-for-Automated-Classification-of-Medical-Images)\n",
    "\n",
    "Data:\n",
    "\n",
    "- RETINAL FUNDUS MULTI-DISEASE IMAGE DATASET (RFMID)\n",
    "[[Reference]](#RETINAL-FUNDUS-MULTI-DISEASE-IMAGE-DATASET-(RFMID))\n",
    "\n",
    "- RFMiD Train Dataset - kaggle\n",
    "[[Reference]](#RFMiD-Train-Dataset---kaggle)\n",
    "\n",
    "Other:\n",
    "\n",
    "- Retinal Image Analysis for multi-Disease Detection Challenge website\n",
    "[[Reference]](#Retinal-Image-Analysis-for-multi-Disease-Detection-Challenge-website)\n",
    "\n",
    "- IEEE ISBI 2021 International Symposium on Biomedical Imaging April 13-16 2021\n",
    "[[Reference]](#IEEE-ISBI-2021-International-Symposium-on-Biomedical-Imaging-April-13-16-2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jIoipX_EL5Ra"
   },
   "source": [
    "## Disclaimer\n",
    "\n",
    "_My experience in trying to develop, debug, train, fit, test etc. models in Google Colab has been not good and less than efficient. I do not mean to say that Google Colab is not good or efficient, it might as well be me doing something wrong or using it in the wrong way._\n",
    "\n",
    "_As a result I have not had sufficient computing power at my disposal for proper hyperparameters._\n",
    "\n",
    "_If I did, I could have used a good `batch number` and calculations such as the ones below to get good numbers for how many `steps` should we perform for training and validation per each of our `epochs` according to the `batches` of our dataset would have been suitable to get good or even state of the art results, and then perform hyperparameter tunning of those and other hyperparameters such as `learning_rate` and others._\n",
    "\n",
    "_As proof of this, please refer to the `screenshots/` directory, located in the [Google Drive Colab Notebooks folder](https://drive.google.com/drive/folders/1wKcqaW1y31s5UXe2BEvwsfcIlvJMKqeK?usp=sharing), mentioned in the previous section and check out my pains, struggles and frustrations with running anything in Google Colab._\n",
    "\n",
    "_Things to look out for in the screenshots:_\n",
    "\n",
    "\n",
    "- _Check out the computer clock and see what time it is in between screenshots (the screenshots are also automatically timestamped) of executing one cell and se **how ridiculously long** it takes for it to execute. And I mean this for normal cells which either process a batch from the dataset or try to train a model, not some cell which is executing something obscure which ends up in an infinite loop or crashes Google Colab._\n",
    "\n",
    "\n",
    "- _Check out how often Google Colab has crashed on me for no particular reason._\n",
    "\n",
    "\n",
    "- _Check out how often the Google Colab session has been taken away from me, before I could save my model or before it has reached a checkpoint so it would save itself._\n",
    "\n",
    "- _Check out how often the Google Colab has become unresponsive for long periods of times, sometimes hours, until I have had to manually interrupt my session, lose everything in my session as a result and have to start over._\n",
    "\n",
    "- _The GPU allocation would be removed as a capability after about an hour worth of working in Google Colab._\n",
    "\n",
    "- _Google Drive and Google Colab synchronization problems._\n",
    "\n",
    "- _... etc._\n",
    "\n",
    "\n",
    "_Because of the above, I have lost more than half of my time for this project in struggling with Google Colab issues, rather than actually working and developing the project._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qoq2XHJL5RZ"
   },
   "source": [
    "## Setup and Running this Notebook\n",
    "\n",
    "_This is a Jupyter Notebook developed and meant to be run in `Google Colab` [[Reference]](#Google-Colab)._\n",
    "\n",
    "_It is quite feature packed and it might take a bit longer to load, depending on the machine on which you are running it on. Please allow sufficient time for all of it to run all the way, until the last LaTeX formula, Markdown, Python, graphs, plots, images, etc. have loaded and executed. This also valid if you use `Kernel -> Restart & Run All`, however in this case this is not meant to be used for this notebook, as some of the cells are Deep Learning model experiments which run for hours, and also running them again will lose some of the results from the experiments._\n",
    "\n",
    "_If you would like to try and run the notebook in an idempotent way I suggest that you comment out the lines of code which fit any models, and only use the lines of code which load pre-trained saved models and saved history._\n",
    "\n",
    "### Requirements\n",
    "\n",
    "- Tensorflow 2.7.0\n",
    "- Keras 2.7.0\n",
    "\n",
    "### Google Drive\n",
    "\n",
    "_Because this notebook is developed as a scientific notebook for an exam project assignment for a Deep Learning course from an Artificial Intelligence module, in order to be assessed you will need to access and download the contents of this shared `Google Drive` folder in order to be able to run it:_\n",
    "\n",
    "___Note: The contents of this folder are around 10GB (around 5GB of which is for the models I have managed to train).___\n",
    "\n",
    "[Google Drive Project Shared folder - softuni-ai-dl-project-2022-paper-2](https://drive.google.com/drive/folders/1mG5QXZIMtxP7V41hPM31wm6cUmy8gh7_?usp=sharing)\n",
    "\n",
    "https://drive.google.com/drive/folders/1mG5QXZIMtxP7V41hPM31wm6cUmy8gh7_?usp=sharing\n",
    "\n",
    "_In case there is a problem with the links above, here is a link to my entire `Google Drive Colab Notebooks folder`:_\n",
    "\n",
    "[Google Drive Colab Notebooks folder](https://drive.google.com/drive/folders/1wKcqaW1y31s5UXe2BEvwsfcIlvJMKqeK?usp=sharing)\n",
    "\n",
    "https://drive.google.com/drive/folders/1wKcqaW1y31s5UXe2BEvwsfcIlvJMKqeK?usp=sharing\n",
    "\n",
    "_The contents should look like this or similar:_\n",
    "\n",
    "```\n",
    ".ipynb_checkpoints/                         --> MacOS folder for Jupyter Notebook checkpoints for this notebook\n",
    "aucmedi-downloads-from-zendo/               --> Incomplete downloads - models download is broken\n",
    "aucmedi-lib/                                --> aucmedi-lib - original unmodified library\n",
    "Evaluation_Set/                             --> Evaluation dataset\n",
    "paper2-experiments-notebook-1-bkps/         --> Backup copies of the experiments notebook\n",
    "paper2-experiments-notebook-1.ipynb         --> The final version of the experiments notebook\n",
    "riadd.aucmedi-repository/                   --> ORIGINAL REPO WITH MODIFIED EXPERIMENTED DEBUGGED SCRIPTS\n",
    "  |__DEBUG-FILES                            --> DIR WITH DEBUG FILES FROM THE MODIFIED SCRIPTS FROM THE REPO\n",
    "Screens1/                                   --> Additional screenshots\n",
    "SoftUni-AI-DL-project-paper-2-2022/         --> PAPER DIRECTORY\n",
    "  |__SoftUni-AI-DL-project-paper-2-2022-VERSION.ipynb  --> FINAL VERSION OF THIS ACTUAL PAPER\n",
    "  |__2103.14660v1-resources/                           --> PAPER RESOURCES\n",
    "  |__SoftUni-AI-DL-project-paper-2-2022-1-bkps/.       --> BACKUP COPIES OF THIS PAPER\n",
    "Training_Set/                               --> Copy of modified dataset for relevant experiments notebook\n",
    "Training_Set-orig-full/                     --> Original full dataset\n",
    "../screenshots/                             --> Screenshots\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Preventable or undiagnosed visual impairment and blindness affect billion of people worldwide. Automated multi-disease detection models offer great potential to address this problem via clinical decision support in diagnosis. In this work, we proposed an innovative multi-disease detection pipeline for retinal imaging which utilizes ensemble learning to combine the predictive capabilities of several heterogeneous deep convolutional neural network models. Our pipeline includes state-of-the-art strategies like transfer learning, class weighting, real-time image augmentation and Focal loss utilization. Furthermore, we integrated ensemble learning techniques like heterogeneous deep learning models, bagging via 5-fold cross-validation and stacked logistic regression models. Through internal and external evaluation, we were able to validate and demonstrate high accuracy and reliability of our pipeline, as well as the comparability with other state-of-the-art pipelines for retinal disease prediction._\n",
    "\n",
    "_The implemented medical image classification pipeline in this paper can be summarized in the following core steps:_\n",
    "\n",
    "- _Stratified multi-label 5-fold cross-validation_\n",
    "\n",
    "- _Class weighted Focal loss and up-sampling_\n",
    "\n",
    "- _Extensive real-time image augmentation_\n",
    "\n",
    "- _Multiple deep learning model architectures_\n",
    "\n",
    "- _Ensemble learning strategies: bagging and stacking_\n",
    "\n",
    "- _Individual training for multi-disease labels and disease risk detection utilizing transfer learning on ImageNet_\n",
    "\n",
    "- _Stacked binary logistic regression models for distinct classification_\n",
    "\n",
    "- _Retinal Imaging Dataset - The RFMiD dataset consists of 3200 retinal images for which 1920 images were used as training dataset. The fundus images were captured by three different fundus cameras having a resolution of 4288x2848 (277 images), 2048x1536 (150 images) and 2144x1424 (1493 images), respectively._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The unique feature of this paper which captured my attention was the innovative use of ensembles, state-of-the-art strategies and pipelines, if you find this summary interesting, please continue reading for the rest of the paper._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___In addition to reading and researching this paper I had the idea that if I get it to run I would like to modify the Neural Network architectures and/or hyperparameters and try to change them in order to improve them.___\n",
    "\n",
    "___This proved next to impossible due to the reasons explained in the [Disclaimer](#Disclaimer) section.___\n",
    "\n",
    "___In addition to that the download with the paper for the original pre-trained models is broken and never finishes no matter how many times I tried it.___\n",
    "\n",
    "___I wanted to modify the Neural Network architectures and/or hyperparameters and try to change them in order to improve them, so I started by reducing the `dataset`, so that it would hopefully run in Google Colab. In reducing the dataset I still made sure to have at least one of each class, as it is a multi-class problem.___\n",
    "\n",
    "_I even created the following dictionary from the data for myself, in order to know where is the first occurrence of each class, to make sure I include at least one on my reduced dataset:_\n",
    "\n",
    "```\n",
    "COLUMN - CLASS = INDEX\n",
    "\n",
    "---\n",
    "\n",
    "C - DR = 2\n",
    "\n",
    "D - ARMD = 7\n",
    "\n",
    "E - MH = 5\n",
    "\n",
    "F - DN = 14\n",
    "\n",
    "G - MYA = 7\n",
    "\n",
    "H - BRVO = 42\n",
    "\n",
    "I - TSLN = 25\n",
    "\n",
    "J - ERM = 10\n",
    "\n",
    "K - LS = 6\n",
    "\n",
    "L - MS = 16\n",
    "\n",
    "M - CSR = 54\n",
    "\n",
    "N - ODC = 5\n",
    "\n",
    "O - CRVO = 59\n",
    "\n",
    "P - TV = 19\n",
    "\n",
    "Q - AH = 113\n",
    "\n",
    "R - ODP = 28\n",
    "\n",
    "S - ODE = 56\n",
    "\n",
    "T - ST = 1222\n",
    "\n",
    "U - AION = 200\n",
    "\n",
    "V - PT = 148\n",
    "\n",
    "W - RT = 36\n",
    "\n",
    "X - RS = 181\n",
    "\n",
    "Y - CRS = 31\n",
    "\n",
    "Z - EDN = 28\n",
    "\n",
    "AA - RPEC = 61\n",
    "\n",
    "AB - MHL = 174\n",
    "\n",
    "AC - RP = 631\n",
    "\n",
    "AD - OTHER = 83\n",
    "```\n",
    "\n",
    "_I did consider doing oversampling, however this is still not implemented for a multi-class problem: https://github.com/scikit-learn-contrib/imbalanced-learn_\n",
    "\n",
    "___With regards as to where to find the relevant files, please refer to the [Google Drive](#Google-Drive) section above.___\n",
    "\n",
    "___Please find the modified dataset in the following directory structure:___\n",
    "\n",
    "```\n",
    "Training_Set/                               --> Copy of modified dataset for relevant experiments notebook\n",
    "```\n",
    "\n",
    "___However I ran into a problem:___\n",
    "\n",
    "_The problem was that despite the fact that I made sure that I have at least one of each class, as it is a multi-class problem, I ended up having exceptions and errors, down the pipeline, stating that in at least one of my samples I have only zeros, meaning I am missing a class:_\n",
    "\n",
    "```\n",
    "line 81, in compute_multilabel_weights\n",
    "    weight = compute_class_weight(class_weight=method, classes=[0,1], y=ohe_array[:, i])\n",
    "\n",
    "    raise ValueError(\"classes should have valid labels that are in y\")\n",
    "ValueError: classes should have valid labels that are in y\n",
    "```\n",
    "\n",
    "_I even managed to trace the error from the ` aucmedi repo scripts --> to the aucmedi library --> to https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/utils/class_weight.py` where the exception is raised, and the exception seems to be raised for the correct reason._ _The mystery remains as to why I get this problem after reducing the dataset in first place._\n",
    "\n",
    "_I did a lot of debugging, generating output in debug files etc. which can be found in the directories below._\n",
    "\n",
    "_Here is the code from <https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/utils/class_weight.py>:_\n",
    "\n",
    "```\n",
    "    elif class_weight == \"balanced\":\n",
    "        # Find the weight of each class as present in y.\n",
    "        le = LabelEncoder()\n",
    "        y_ind = le.fit_transform(y)\n",
    "        if not all(np.in1d(classes, le.classes_)):\n",
    "            raise ValueError(\"classes should have valid labels that are in y\")\n",
    "```\n",
    "\n",
    "_On the basis of that code, here is some of my debug code in this script `classifier_DenseNet201.py`:_\n",
    "\n",
    "```\n",
    "    classes=[0,1]\n",
    "    for j in range(0, np.shape(y_train)[1]):\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        y_ind = le.fit_transform(y_train[:, j])\n",
    "        # if not all(np.in1d(classes, le.classes_)):\n",
    "        if not all(np.in1d(classes, le.classes_)):\n",
    "            print(\">>> error:\", classes, \"not in\", le.classes_)\n",
    "            print(\">>> i:\" , i)\n",
    "            print(\">>> j:\" , j)\n",
    "            with np.printoptions(threshold=np.inf):\n",
    "                with open('error1.txt', 'w') as f:\n",
    "                    print(\">>> error:\", classes, \"not in\", le.classes_, file=f)\n",
    "                    print(\">>> i:\" , i, file=f)\n",
    "                    print(\">>> j:\" , j, file=f)\n",
    "                    print(\">>> y_train[:, j]: \", y_train[:, j], file=f)\n",
    "```\n",
    "\n",
    "_And here are some output from my debug code, confirming that indeed I end up with `0`s in the data:_\n",
    "\n",
    "```\n",
    ">>> y_train shape:  (205, 28)\n",
    ">>> error: [0, 1] not in [0]\n",
    ">>> i: 17\n",
    ">>> error: [0, 1] not in [0]\n",
    ">>> i: 19\n",
    ">>> error: [0, 1] not in [0]\n",
    ">>> i: 26\n",
    "Traceback (most recent call last):\n",
    "  File \"scripts/classifier_DenseNet201.py\", line 228, in <module>\n",
    "    class_weights = compute_multilabel_weights(ohe_array=y_train)\n",
    "  File \"/usr/local/lib/python3.7/dist-packages/aucmedi/utils/class_weights.py\", line 81, in compute_multilabel_weights\n",
    "    y=ohe_array[:, i])\n",
    "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/class_weight.py\", line 50, in compute_class_weight\n",
    "    raise ValueError(\"classes should have valid labels that are in y\")\n",
    "ValueError: classes should have valid labels that are in y\n",
    "```\n",
    "\n",
    "___Please find the modified scripts in:___\n",
    "\n",
    "```\n",
    "riadd.aucmedi-repository/                   --> ORIGINAL REPO WITH MODIFIED EXPERIMENTED DEBUGGED SCRIPTS\n",
    "  |__DEBUG-FILES                            --> DIR WITH DEBUG FILES FROM THE MODIFIED SCRIPTS FROM THE REPO\n",
    "```\n",
    "\n",
    "___Please see this script in particular:___\n",
    "\n",
    "```\n",
    "riadd.aucmedi-repository/\n",
    "  |__scripts/\n",
    "      |__classifier_DenseNet201.py\n",
    "```\n",
    "\n",
    "_I have done most of the code modifications for debugging of aforementioned problem in there. Please keep in mind this is only debug code, so it is not tidied up._\n",
    "\n",
    "___Despite all of the aforementioned problems I did manage to do one complete experiment run, but with only some classifiers and detectors, not all due to the reasons explained in the [Disclaimer](#Disclaimer) section.___ \n",
    "\n",
    "___In order to do that I had to overcome the problem explained above, and I achieved it by replacing the loss function in the `classifier_DenseNet201.py` script:___\n",
    "\n",
    "___I replaced this:___\n",
    "\n",
    "```\n",
    "#    loss=multilabel_focal_loss(class_weights),\n",
    "```\n",
    "\n",
    "___with:___\n",
    "\n",
    "```\n",
    "loss='categorical_crossentropy',\n",
    "```\n",
    "\n",
    "___With that I did get around the error, but the results did not look good which was to be expected.___\n",
    "\n",
    "___Please see the following directories and files for the results of this experiment:___\n",
    "\n",
    "```\n",
    "riadd.aucmedi-repository/\n",
    "  |__models\n",
    "  |__preds\n",
    "```\n",
    "\n",
    "___An additional problems were the sacrifices I have had to make and reduce our hyperparameters with Google Colab training due to the reasons explained in the [Disclaimer](#Disclaimer) section.___\n",
    "\n",
    "___For additional context and visual representation of the problems explained above please see the screenshots in the following directories:___\n",
    "\n",
    "```\n",
    "Screens1/                                   --> Additional screenshots\n",
    "../screenshots/                             --> Screenshots\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Foreword\n",
    "\n",
    "_One of the aims of this article is to understand some Deep Learning (DL) basics, basic concepts and intuitions._\n",
    "\n",
    "_Because of that goal it is important to be able to train and test models multiple times, so we can determine the best hyperparameters and tune models to improve them. Unfortunately at the time of writing this article, in the year 2022, I am using my personal laptop from 2015. I thought this machine has fared rather well for it's age, and have great respect for it and what I have put it through, as we have been together through thick and thin. That was until I had to actually do DL on it for this article when I realized it is not going to fare well for this purpose. Most of the models were unable to run or finish running once I tried to train them, or once I tried to change the hyperparameters to improve them. My machine would just heat up with fans running at the highest rpms and still seem stuck on executing a cell for more than 30min. If I had carried on like this I would not have been able to finish this article, so instead most or all of the hyperparameters for the models are set to severely low or high, depending on the context, in order to reduce iterations or features of the data, so that this notebook would work and I would be able to demonstrate or give an example of the idea I am trying to explain. Please keep this in mind when going through the article._\n",
    "\n",
    "_This will be sufficient for the purpose of this article, just to demonstrate and help understand DL basics, basic concepts and intuitions, but if you need to try out some of the examples and extend and improve them for actual Deep Learning keep in mind you need a powerful machine with a good GPU, or you can use a cloud platform suitable for DL, such as Google Colab._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References Notes\n",
    "\n",
    "_Any and all references, citations, resources or other materials used to understand and explain, provide examples, and build this article have been referenced in order to give credit where credit is due and avoid plagiarism._\n",
    "_If a citation is the bigger part of a section, and has been edited, added to, modified, etc. the reference to that section would be at the end of it, separated with a horizontal line, like this example:_\n",
    "\n",
    "> ---\n",
    "> [[Example Reference]](#ExampleReference)\n",
    "\n",
    "_If a citation has been inserted and is relatively short, the relevant reference will be at the end of the sentence or paragraph, for example:_\n",
    "\n",
    "> Example. [[Example Reference]](#ExampleReference)\n",
    "\n",
    "_In case a reference is missed due to human error, all references can be found in the [References](#References) section. Anything which is found in the [References](#References) should be considered as a valid reference for everything in this paper, even if not explicitly referenced._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Narrative\n",
    "\n",
    "_I have tried to provide a nice flow, ease of read and a friendly and humorous tone of the article, and at the same time clear and understandable communication. In order to aid this I have provided a narrative to this article. In order to distinguish it I have used italics for it throughout the article. Please consider any text in italics, such as the one you are currently reading, as narrative. It can also be both in bold and italics._\n",
    "\n",
    "> _Example narrative._\n",
    "\n",
    "### Code\n",
    "\n",
    "_Currently most of the code in the article has been refactored into separate functions and most of the other code in the article is left fragmented throughout. There is a very good reason for this, which is that one of the aims of this article is to also understand a bit of Deep Learning. This is why the fragments of code throughout this article are used to help us and illustrate and demonstrate different parts of ML as a whole._\n",
    "\n",
    "_Some of the code quality has been improved by making some functions idempotent with special checks, so that they have the same effect, no matter how many times they are ran._\n",
    "\n",
    "_Most of the commented out code in this article is left on purpose to serve as information, as part of the intent for this article is for it to be a knowledgebase._\n",
    "\n",
    "### Table of Contents (TOC)\n",
    "\n",
    "_Please refer to the [Table of Contents](#Table-of-Contents) section in [Appendix A](#Appendix-A) for instructions on how you can use get a Table of Contents for this article in Jupyter Notebook._\n",
    "\n",
    "### Running this Jupyter Notebook\n",
    "\n",
    "_This Jupyter Notebook is quite feature packed and it might take a bit longer to load, depending on the machine on which you are running it on. Please allow sufficient time for all of it to run all the way, until the last LaTeX formula, Markdown, Python, graphs, plots, images, etc. have loaded and executed. This also valid if you use `Kernel -> Restart & Run All`._\n",
    "\n",
    "### Testing\n",
    "\n",
    "#### Project tests\n",
    "\n",
    "- _Any mathematics in the project for which I have had doubts or have not understood I have tested using Wolfram Alpha._\n",
    "\n",
    "- _I have repeatedly ran \"Kernel -> Restart & Run All\" to confirm all is working and have fixed bugs when things have been broken._\n",
    "\n",
    "#### Code tests\n",
    "\n",
    "- _There are code test, however the focus of this notebook is not on code tests. Due to the nature of this notebook, being focused on ML, most of the tests of this note book are actually metrics, scoring, score analysis, model testing and cross-validation._\n",
    "\n",
    "- _There are tests in the project. Since code tests are outside of the focus of this project most of the tests are visual print outs of the data and visual confirmations._\n",
    "\n",
    "- _Most of the tests in this project are visual and are marked with this \"`### Test`\" comment above it._\n",
    "\n",
    "- _There are also tests which are more functional and for example print a message if an assertion error is not thrown._\n",
    "\n",
    "_I consider this amount of test coverage adequate for the purpose of this article._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preventable or undiagnosed visual impairment and blindness affect\n",
    "billion of people worldwide. Automated multi-disease detection models\n",
    "offer great potential to address this problem via clinical decision\n",
    "support in diagnosis. In this work, we proposed an innovative\n",
    "multi-disease detection pipeline for retinal imaging which utilizes\n",
    "ensemble learning to combine the predictive capabilities of several\n",
    "heterogeneous deep convolutional neural network models. Our pipeline\n",
    "includes state-of-the-art strategies like transfer learning, class\n",
    "weighting, real-time image augmentation and Focal loss utilization.\n",
    "Furthermore, we integrated ensemble learning techniques like\n",
    "heterogeneous deep learning models, bagging via 5-fold cross-validation\n",
    "and stacked logistic regression models. Through internal and external\n",
    "evaluation, we were able to validate and demonstrate high accuracy and\n",
    "reliability of our pipeline, as well as the comparability with other\n",
    "stateof-the-art pipelines for retinal disease prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Index Terms---*** Retinal Disease Detection, Ensemble Learning,\n",
    "> Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. INTRODUCTION**\n",
    "\n",
    "Even if the medical progress in the last 30 years made it possible to\n",
    "successfully treat the majority of diseases causing visual impairment,\n",
    "growing and aging populations lead to an increasing challenge in retinal\n",
    "disease diagnosis \\[1\\]. The World Health Organization (WHO) estimates\n",
    "the prevalence of blindness and visual impairment to 2.2 billion people\n",
    "worldwide, of whom at least 1 billion affections could have been\n",
    "prevented or is yet to be addressed \\[2\\]. Early detection and correct\n",
    "diagnosis are essential to forestall disease course and prevent\n",
    "blindness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of clinical decision support (CDS) systems for diagnosis has\n",
    "been increasing over the past decade \\[3\\]. Recently, modern deep\n",
    "learning models allow automated and reliable classification of medical\n",
    "images with remarkable accuracy comparable to physicians \\[4\\].\n",
    "Nevertheless, these models often lack capabilities to detect rare\n",
    "pathologies such as central retinal artery occlusion or anterior\n",
    "ischemic optic neuropathy \\[5\\], \\[6\\]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this study we push towards creating a highly accurate and reliable\n",
    "multi-disease detection pipeline based on ensemble, transfer and deep\n",
    "learning techniques. Furthermore, we utilize the new Retinal Fundus\n",
    "Multi-Disease Image Dataset (RFMiD) containing various rare and\n",
    "challenging conditions to demonstrate our detection capabilities for\n",
    "uncommon diseases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. METHODS**\n",
    "\n",
    "The implemented medical image classification pipeline can be\n",
    "summarized in the following core steps and is illustrated in Fig. 1:\n",
    "- Stratified multi-label 5-fold cross-validation\n",
    "- Class weighted Focal loss and up-sampling\n",
    "- Extensive real-time image augmentation\n",
    "- Multiple deep learning model architectures\n",
    "- Ensemble learning strategies: bagging and stacking\n",
    "- Individual training for multi-disease labels and disease risk\n",
    "detection utilizing transfer learning on ImageNet\n",
    "- Stacked binary logistic regression models for distinct\n",
    "classification\n",
    "**2.1. Retinal Imaging Dataset**\n",
    "The RFMiD dataset consists of 3200 retinal images for which 1920\n",
    "images were used as training dataset \\[7\\]. The fundus images were\n",
    "captured by three different fundus cameras having a resolution of\n",
    "4288x2848 (277 images), 2048x1536 (150 images) and 2144x1424 (1493\n",
    "images), respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tab. 1.** Annotation frequency for each class in the dataset.\n",
    "\n",
    "![Tab1](2103.14660v1-resources/images/Tab1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image1](2103.14660v1-resources/2103.14660v1-paper-images/image1.png)\n",
    "\n",
    "**Fig. 1**. Flowchart diagram of the implemented medical image\n",
    "analysis pipeline for multi-disease detection in retinal imaging. The\n",
    "workflow is starting with the retinal imaging dataset (RFMiD) and ends\n",
    "with computed predictions for novel images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images were annotated with 46 conditions, including various rare and\n",
    "challenging diseases, through adjudicated consensus of two senior\n",
    "retinal experts. These 46 conditions are represented by the following\n",
    "classes, which are also listed in Tab. 1: An overall normal/abnormal\n",
    "class, 27 specific condition classes and 1 'OTHER' class consisting of\n",
    "the remaining extremely rare conditions. Besides the training dataset,\n",
    "the organizers of the RIADD challenge hold 1280 images back for external\n",
    "validation and testing datasets to ensure robust evaluation \\[7\\],\n",
    "\\[8\\]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2. Preprocessing and Image Augmentation**\n",
    "\n",
    "In order to simplify the pattern finding process of the deep learning\n",
    "model, as well as to increase data variability, we applied several\n",
    "preprocessing methods.\n",
    "\n",
    "We utilized extensive image augmentation for up-sampling to balance class distribution and real-time augmentation during\n",
    "training to obtain novel and unique images in each epoch. The\n",
    "augmentation techniques consisted of rotation, flipping, and altering in\n",
    "brightness, saturation, contrast and hue. Through the up-sampling, it\n",
    "was ensured that each label occurred at least 100 times in the dataset\n",
    "which increased the total number of training images from 1920 to 3354.\n",
    "\n",
    "Afterwards, all images were square padded in order to avoid aspect ratio\n",
    "loss during posterior resizing. The retinal images were also cropped to\n",
    "ensure that the fundus is center located in the image. The cropping was\n",
    "performed individually for each microscope resolution and resulted in\n",
    "the following image shapes: 1424x1424, 1536x1536 and 3464x3464 pixels.\n",
    "The images were then resized to model input sizes according to the neural network architecture, which was\n",
    "380x380 for EfficientNetB4, 299x299 for InceptionV3 and 244x244 for\n",
    "all remaining architectures \\[9\\]--\\[12\\].\n",
    "\n",
    "Before feeding the image to the deep convolutional neural network, we\n",
    "applied value intensity normalization as last preprocessing step. The\n",
    "intensities were zero-centered via the Z-Score normalization approach\n",
    "based on the mean and standard deviation computed on the ImageNet\n",
    "dataset \\[13\\]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.3. Deep Learning Models**\n",
    "\n",
    "The state-of-the-art for medical image classification are the\n",
    "unmatched deep convolutional neural network models \\[4\\], \\[14\\].\n",
    "Nevertheless, the hyper parameter configuration and architecture\n",
    "selection are highly dependent on the required computer vision task,\n",
    "as well as the key difference between pipelines \\[4\\], \\[15\\]. Thus,\n",
    "our pipeline combines two different types of image classification\n",
    "models: The disease risk detector for binary classifying\n",
    "normal/abnormal images and the disease label classifier for\n",
    "multi-label annotation of abnormal images.\n",
    "\n",
    "Both model types were pretrained on the ImageNet\n",
    "\n",
    "dataset \\[13\\]. For the fitting process, we applied a transfer\n",
    "learning training, with frozen architecture layers except for the\n",
    "classification head, and a fine-tuning strategy with unfrozen layers.\n",
    "Whereas the transfer learning fitting was performed for 10 epochs\n",
    "using the Adam optimization with an initial learning rate of 1-E04,\n",
    "the fine-tuning had a maximal training time of 290 epochs and using a\n",
    "dynamic learning rate for the Adam optimization starting from 1-E05 to\n",
    "a maximum decrease to 1-E07 (decreasing factor of 0.1 after 8 epochs without improvement on the monitored validation loss)\n",
    "\\[16\\]. Furthermore, an early stopping and model checkpoint technique\n",
    "was utilized for the fine-tuning process, stopping after 20 epochs\n",
    "without improvement (after epoch 60) and saving the best model measured\n",
    "according to the validation loss. Instead of defining an epoch as a\n",
    "cycle through the full training dataset, we establish an epoch to have\n",
    "250 iterations. This allowed to increase the number of seen batches and,\n",
    "thus, to increase the information given to the model during the fitting\n",
    "process of an epoch. As training loss function, we utilized the weighted\n",
    "Focal loss from *Lin et al.* \\[17\\]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$FL(𝑝𝑡) = −𝛼𝑡(1 − 𝑝𝑡)𝛾 log (𝑝𝑡) (1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above formula, *pt* is the probability for the correct ground\n",
    "truth class *t*, *γ* a tunable focusing parameter (which we set to 2.0)\n",
    "and *αt* the associated weight for class *t*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *2.3.1 Disease Risk Detector*\n",
    "\n",
    "The disease risk detector was established as a binary classifier of the\n",
    "disease risk class for general categorizing between normal and abnormal\n",
    "retinal images. Thus, this model type was trained using only the disease\n",
    "risk class and ignoring all multi-label annotations. Rather than using a\n",
    "single model architecture, we trained multiple models based on the\n",
    "DenseNet201 and EfficientNetB4 architecture \\[9\\], \\[10\\]. For class\n",
    "weight computation, we divided the number of samples by the\n",
    "multiplication of the number of classes (2 for a binary classification)\n",
    "with the number of class occurrences in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *2.3.2 Disease Label Classifier*\n",
    "\n",
    "In contrast, the disease label classifier was established as multi-label\n",
    "classifier of all 28 remaining classes (excluding disease risk) and was\n",
    "trained on the one hot encoded array of the disease labels. Furthermore,\n",
    "we utilized four different architectures for this model type: ResNet152,\n",
    "InceptionV3, DenseNet201 and EfficientNetB4 \\[9\\]--\\[12\\]. Identical to\n",
    "class weight computation of the disease risk detector, we computed the\n",
    "weights individually as binary classification for each class. Even if\n",
    "this classifier is provided with all classes, the binary weights balance\n",
    "the decision for each label individually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.4. Ensemble Learning Strategy**\n",
    "\n",
    "#### *2.4.1 Bagging*\n",
    "\n",
    "Next to the utilization of multiple architecture, we also applied a\n",
    "5-fold cross-validation based as a bagging approach for ensemble\n",
    "learning. Our aim was to create a large variety of models which were\n",
    "trained on different subsets of the training data. This approach not\n",
    "only allowed a more efficient usage of the available training data, but\n",
    "also increased the reliability of a prediction. This strategy resulted in an ensemble of\n",
    "10 disease risk detector models (2 architectures with each 5 folds)\n",
    "and 20 disease label classifier models (4 architectures with each 5\n",
    "folds)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *2.4.2 Stacking*\n",
    "\n",
    "For combining the predictions of our, in total, 30 models, we\n",
    "integrated a stacking setup. On top of all deep convolutional neural\n",
    "networks, we applied a binary logistic regression algorithm for each\n",
    "class, individually. Thus, the predictions of all models were utilized\n",
    "as input for computing the classification of a single class. This\n",
    "approach allowed combining the information of all other class\n",
    "predictions to derive an inference for one single class. Overall, this\n",
    "strategy resulted in 29 distinct logistic regression models (1 for\n",
    "disease risk and 28 for each disease-label including the 'other'\n",
    "class). The individual predicted class probabilities are then\n",
    "concatenated to the final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression models were also trained with the same 5-fold\n",
    "cross-validation sampling on a heavily augmented version of the\n",
    "training dataset to avoid overfitting as well as avoiding training the\n",
    "logistic regression models on already seen images from the neural\n",
    "network models. As logistic regression solver, we utilized the\n",
    "large-scale boundconstrained optimization (short: 'LBFGS') from *Zhu\n",
    "et al*. \\[18\\]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. RESULTS AND DISCUSSION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequential training of a complete cross-validation for one\n",
    "architecture on a single NVIDIA TITAN RTX GPU took around 13.5 hours\n",
    "with 63 epochs on average for each deep convolutional neural network\n",
    "model. Logistic Regression training required less than 30 minutes for\n",
    "all class models combined. No signs of overfitting were observed for\n",
    "the disease label classifiers through validation monitoring, as it can\n",
    "be seen in Fig. 2. However, the disease risk detectors showed a strong\n",
    "trend to overfit after the transfer learning phase. Through our\n",
    "strategy to use the model with the best validation loss, it was still possible to obtain a powerful model for\n",
    "detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image2](2103.14660v1-resources/2103.14660v1-paper-images/image2.png)\n",
    "\n",
    "\n",
    "**Fig. 2.** Loss course during the\n",
    "training process for training and validation data. The lines were\n",
    "computed via locally estimated scatterplot smoothing and represent the\n",
    "average loss across all folds. The gray areas around the lines\n",
    "represent the confidence intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image3](2103.14660v1-resources/2103.14660v1-paper-images/image3.png)\n",
    "\n",
    "**Fig. 3.** Receiver operating characteristic (ROC) curves for each\n",
    "model type applied in our pipeline. The ROC curves showing the\n",
    "individual model performance measured by the true positive and false\n",
    "positive rate. The cross-validation models were macro-averaged for each\n",
    "model type to reduce illustration complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.1. Internal Performance Evaluation**\n",
    "\n",
    "For estimating the performance of our pipeline, we utilized the\n",
    "validation subsets of the 5-fold cross-validation models from the\n",
    "heavily augmented version of our dataset. This approach allowed to\n",
    "obtain testing samples which were never seen in the training process for\n",
    "reliable performance evaluation. For the complex multi-label evaluation,\n",
    "we computed the popular area under the receiver operating characteristic\n",
    "(AUROC) curve, as well as the mean average precision (mAP). Both scores\n",
    "were macro-averaged over classes and cross-validation folds to reduce\n",
    "complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our multi-disease detection pipeline revealed a strong and robust\n",
    "classification performance with the capability to also detect rare\n",
    "conditions accurately in retinal images. Whereas the disease label\n",
    "classifier models separately only achieved an AUROC of around 0.97 and a\n",
    "mAP of 0.93, the disease risk detectors demonstrated to have a really\n",
    "strong predictive power of 0.98 up to 0.99 AUROC and mAP. However, for\n",
    "the classifiers the InceptionV3 architecture indicated to have the worst\n",
    "performance compared to the other architectures with only 0.93 AUROC and\n",
    "0.66 mAP. The associated receiver operating characteristics of the\n",
    "models are illustrated in Fig. 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a strong multi-label classifier is in general a complex task,\n",
    "however, the extreme class imbalance between the conditions revealed a\n",
    "hard challenge for building a reliable model \\[19\\], \\[20\\]. Our applied\n",
    "up-sampling and class weighting technique demonstrated to have a\n",
    "critical boost on the predictive capabilities of the classifier models.\n",
    "Nearly all labels were able to be accurately detected, including the 'OTHER' class consisting of various extremely rare\n",
    "conditions. Nevertheless, the two classes 'EDN' and 'CRS' were the\n",
    "most challenging conditions for all classifier models. Both classes\n",
    "belong to very rare conditions, combined with less than 1.2%\n",
    "occurrence in the original and 2.5% occurrence in the up-sampled\n",
    "dataset. Still, our stacked logistic regression algorithm was able to\n",
    "balance this issue and infer the correct 'EDN' and 'CRS'\n",
    "classifications through context. Overall, our applied ensemble\n",
    "learning strategies resulted in a significant performance improvement\n",
    "compared to the individual deep convolutional neural network models.\n",
    "\n",
    "More details on the internal performance evaluation are listed in Tab. 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2. External Evaluation through the RIADD Challenge** \n",
    "\n",
    "Furthermore, we participated at the RIADD challenge which was organized by the\n",
    "authors of the RFMiD dataset \\[7\\], \\[8\\]. The challenge participation\n",
    "allowed not only an independent evaluation of the predictive power of our pipeline on an unseen and\n",
    "unpublished testing set, but also the comparison with the currently best\n",
    "retinal disease classifiers in the world.\n",
    "\n",
    "![Tab2](2103.14660v1-resources/images/Tab2.png)\n",
    "\n",
    "**Tab. 2**. Achieved results of the internal performance evaluation\n",
    "showing the average AUROC and mAP score for each model utilized in our\n",
    "pipeline. The scores were macroaveraged across all cross-validation\n",
    "folds and classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our participation, we were able to reach rank 19 from a total of 59\n",
    "teams in the first evaluation phase and rank 8 in the final phase. In\n",
    "the independent evaluation from the challenge organizers, we achieved an\n",
    "AUROC of 0.95 for the disease risk classification. For multi-label\n",
    "scoring, they computed the average between the macro-averaged AUROC and\n",
    "the mAP, for which we reached the score 0.70. The top performing ranks\n",
    "shared only a marginal scoring difference which is why we had only a\n",
    "final score difference of 0.05 to the first ranked team. Furthermore,\n",
    "the participation results demonstrated that ensemble learning based\n",
    "classification for deep convolutional neural network models is\n",
    "compatible or even superior to other approaches in the scientific field\n",
    "such as focusing on a single large architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.3. Experiments and Improvements**\n",
    "\n",
    "Additionally, we experimented with using weighted crossentropy loss for\n",
    "training our both model types. This resulted in inferior models for\n",
    "disease label classification, however, the cross-entropy loss fitted\n",
    "disease risk detector models showed less overfitting with equal\n",
    "performance. Further experimentation with loss functions for the disease\n",
    "risk detector models could provide the solution to avoid overfitting.\n",
    "\n",
    "An important point for the RIADD challenge participation would be the utilization of more training data, especially\n",
    "for the difficult 'CRS' and 'EDN' classes. According to the challenge\n",
    "rules, other public available datasets like Kaggle DR, IDRiD, Messidor\n",
    "or APTOS are allowed to be used as additional training data \\[8\\]. Our\n",
    "pipeline, which was trained exclusively on the RFMiD dataset, could be\n",
    "further improved with more retinal images of very rare conditions.\n",
    "Besides the training data, more improvement points for further research\n",
    "in retinal disease detection would be the inclusion of image cropping\n",
    "strategies to reduce information loss through resolution resizing, the\n",
    "usage of more architectures (especially with different input\n",
    "resolutions) to increase the model ensemble, and the utilization of\n",
    "specific retinal filters or retinal vessel segmentation as additional\n",
    "information to utilize for the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. CONCLUSIONS**\n",
    "\n",
    "In this study, we introduced a powerful multi-disease detection pipeline\n",
    "for retinal imaging which exploits ensemble learning techniques to\n",
    "combine the predictions of various deep convolutional neural network\n",
    "models. Next to state-of-the-art strategies, such as transfer learning,\n",
    "class weighting, extensive real-time image augmentation and Focal loss\n",
    "utilization, we applied 5-fold cross-validation as bagging technique and\n",
    "used multiple convolutional neural network architectures to create an ensemble of models. With a stacking\n",
    "approach of class-wise distinct logistic regression models, we\n",
    "combined the knowledge of all neural network models to compute highly\n",
    "accurate and reliable retinal condition predictions. Next to an\n",
    "internal performance evaluation, we also proved the precision and\n",
    "comparability of our pipeline through the participation at the RIADD\n",
    "challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **APENDIX**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to ensure full reproducibility and to create a base for\n",
    "further research, the complete code of this study, including extensive\n",
    "documentation, is available in the following public Git repository:\n",
    "https://github.com/frankkramer-lab/riadd.aucmedi\n",
    "Furthermore, the trained models, evaluation results and metadata are\n",
    "available in the following public Zenodo repository:\n",
    "https://doi.org/10.5281/zenodo.4573990"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **ACKNOWLEDGMENTS**\n",
    "\n",
    "We want to thank Dennis Klonnek, Edmund Müller and Johann Frei for\n",
    "their useful comments and support.\n",
    "\n",
    "## **COMPLIANCE WITH ETHICAL STANDARDS**\n",
    "\n",
    "This research study was conducted retrospectively using human subject\n",
    "data made available in open access by *Pachade et al.* \\[7\\], \\[8\\].\n",
    "Ethical approval was not required as confirmed by the license attached\n",
    "with the open access data.\n",
    "\n",
    "## **CONFLICT OF INTEREST**\n",
    "\n",
    "None declared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **FUNDING**\n",
    "\n",
    "This work is a part of the DIFUTURE project funded by the German\n",
    "Ministry of Education and Research (Bundesministerium für Bildung und\n",
    "Forschung, BMBF) grant FKZ01ZZ1804E."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References from original paper <a id=\"ReferencesOGPaper\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] J. D. Adelson et al., “Causes of blindness and vision impairment\n",
    "in 2020 and trends over 30 years, and prevalence of avoidable\n",
    "blindness in relation to VISION 2020: the Right to Sight: an\n",
    "analysis for the Global Burden of Disease Study,” Lancet Glob.\n",
    "Heal., vol. 9, no. 2, pp. e144–e160, Feb. 2021, doi:\n",
    "10.1016/S2214-109X(20)30489-7.\n",
    "\n",
    "[2] World Health Organization, “Blindness and vision impairment.”\n",
    "https://www.who.int/news-room/fact-sheets/detail/blindness-andvisual-impairment (accessed Feb. 27, 2021).\n",
    "\n",
    "[3] R. T. Sutton, D. Pincock, D. C. Baumgart, D. C. Sadowski, R. N.\n",
    "Fedorak, and K. I. Kroeker, “An overview of clinical decision\n",
    "support systems: benefits, risks, and strategies for success,” npj\n",
    "Digital Medicine, vol. 3, no. 1. Nature Research, pp. 1–10, Dec. 01, 2020, doi: 10.1038/s41746-020-0221-y\n",
    "\n",
    "[4] G. Litjens et al., “A survey on deep learning in medical image\n",
    "analysis,” Med. Image Anal., vol. 42, no. December 2012, pp. 60\n",
    "–\n",
    "88, 2017, doi: 10.1016/j.media.2017.07.005.\n",
    "\n",
    "[5] J. Y. Choi, T. K. Yoo, J. G. Seo, J. Kwak, T. T. Um, and T. H.\n",
    "Rim, “Multi\n",
    "-categorical deep learning neural network to classify\n",
    "retinal images: A pilot study employing small database,” PLoS\n",
    "One, vol. 12, no. 11, p. e0187336, Nov. 2017, doi:\n",
    "10.1371/journal.pone.0187336.\n",
    "\n",
    "[6] G. Quellec, M. Lamard, P. H. Conze, P. Massin, and B. Cochener,\n",
    "“Automatic detection of rare pathologies in fundus photographs\n",
    "using few\n",
    "-shot learning,” Med. Image Anal., vol. 61, p. 101660,\n",
    "Apr. 2020, doi: 10.1016/j.media.2020.101660.\n",
    "\n",
    "[7] S. Pachade et al., “Retinal Fundus Multi\n",
    "-Disease Image Dataset\n",
    "(RFMiD): A Dataset for Multi\n",
    "-Disease Detection Research,” Data,\n",
    "vol. 6, no. 2, p. 14, Feb. 2021, doi: 10.3390/data6020014.\n",
    "\n",
    "[8] “Home\n",
    "- RIADD (ISBI\n",
    "-2021)\n",
    "- Grand Challenge.”\n",
    "https://riadd.grand\n",
    "-challenge.org/Home/ (accessed Feb. 27, 2021).\n",
    "\n",
    "[9] G. Huang, Z. Liu, L. van der Maaten, and K. Q. Weinberger,\n",
    "“Densely Connected Convolutional Networks,” Proc.\n",
    "- 30th IEEE\n",
    "Conf. Comput. Vis. Pattern Recognition, CVPR 2017, vol. 2017\n",
    "-\n",
    "January, pp. 2261\n",
    "–2269, Aug. 2016, Accessed: Feb. 27, 2021.\n",
    "[Online]. Available: http://arxiv.org/abs/1608.06993.\n",
    "\n",
    "[10] M. Tan and Q. V. Le, “EfficientNet: Rethinking Model Scaling for\n",
    "Convolutional Neural Networks,” 36th Int. Conf. Mach. Learn.\n",
    "ICML 2019, vol. 2019\n",
    "-June, pp. 10691\n",
    "–10700, May 2019,\n",
    "Accessed: Feb. 27, 2021. [Online]. Available:\n",
    "http://arxiv.org/abs/1905.11946.\n",
    "\n",
    "[11] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna,\n",
    "“Rethinking the Inception Architecture for Computer Vision,” in\n",
    "Proceedings of the IEEE Computer Society Conference on\n",
    "Computer Vision and Pattern Recognition, Dec. 2016, vol. 2016\n",
    "-\n",
    "December, pp. 2818\n",
    "–2826, doi: 10.1109/CVPR.2016.308.\n",
    "\n",
    "[12] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for\n",
    "image recognition,” in Proceedings of the IEEE Computer Society\n",
    "Conference on Computer Vision and Pattern Recognition, Dec.\n",
    "2016, vol. 2016\n",
    "-December, pp. 770\n",
    "–778, doi:\n",
    "10.1109/CVPR.2016.90.\n",
    "\n",
    "[13] O. Russakovsky et al., “ImageNet Large Scale Visual Recognition\n",
    "Challenge,” Int. J. Comput. Vis., vol. 115, no. 3, pp. 211\n",
    "–252, Dec.\n",
    "2015, doi: 10.1007/s11263\n",
    "-015\n",
    "-0816\n",
    "-y.\n",
    "\n",
    "[14] S. Muhammad et al., “Medical Image Analysis using\n",
    "Convolutional Neural Networks A Review,” J. Med. Syst., vol. 42,\n",
    "no. 11, pp. 1\n",
    "–13, Nov. 2018, doi: 10.1007/s10916\n",
    "-018\n",
    "-1088\n",
    "-1.\n",
    "\n",
    "[15] J. Ker, L. Wang, J. Rao, and T. Lim, “Deep Learning Applications\n",
    "in Medical Image Analysis,” IEEE Access, vol. 6, pp. 9375\n",
    "–9379,\n",
    "2017, doi: 10.1109/ACCESS.2017.2788044.\n",
    "\n",
    "[16] D. P. Kingma and J. Lei Ba, “Adam: A Method for Stochastic\n",
    "Optimization,” 2014. https://arxiv.org/abs/1412.6980.\n",
    "\n",
    "[17] T.\n",
    "-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollár, “Focal Loss\n",
    "for Dense Object Detection,” IEEE Trans. Pattern Anal. Mach.\n",
    "Intell., vol. 42, no. 2, pp. 318\n",
    "–327, Aug. 2017, Accessed: Feb. 27,\n",
    "2021. [Online]. Available: http://arxiv.org/abs/1708.02002.\n",
    "\n",
    "[18] C. Zhu, R. H. Byrd, P. Lu, and J. Nocedal, “L\n",
    "-BFGS\n",
    "-B: Fortran\n",
    "Subroutines for Large\n",
    "-Scale Bound\n",
    "-Constrained Optimization,”\n",
    "ACM Trans. Math. Softw., vol. 23, no. 4, pp. 550\n",
    "–560, Dec. 1997,\n",
    "doi: 10.1145/279232.279236.\n",
    "\n",
    "[19] P. Kaur and A. Gosain, “Issues and challenges of class imbalance\n",
    "problem in classification,” Int. J. Inf. Technol., pp. 1\n",
    "–7, Oct. 2020,\n",
    "doi: 10.1007/s41870\n",
    "-018\n",
    "-0251\n",
    "-8.\n",
    "\n",
    "[20] L. Gao, L. Zhang, C. Liu, and S. Wu, “Handling imbalanced\n",
    "medical image data: A deep\n",
    "-learning\n",
    "-based one\n",
    "-class\n",
    "classification approach,” Artif. Intell. Med., vol. 108, p. 101935,\n",
    "Aug. 2020, doi: 10.1016/j.artmed.2020.101935."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "*In order to use a Table of Contents for this article, please use the `toc2` extension from `Nbextensions` for Jupyter Notebook. You can find instructions on how to install and use it in this <a href=\"https://stackoverflow.com/questions/21151450/how-can-i-add-a-table-of-contents-to-a-jupyter-jupyterlab-notebook\">link</a>.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References <a id=\"ReferencesSection\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### MULTI-DISEASE DETECTION IN RETINAL IMAGING PDF\n",
    "<https://arxiv.org/pdf/2103.14660v1.pdf>\n",
    "\n",
    "### Multi-Disease Detection in Retinal Imaging - papers with code\n",
    "<https://paperswithcode.com/paper/multi-disease-detection-in-retinal-imaging>\n",
    "\n",
    "### Multi-Disease Detection in Retinal Imaging - GitHub\n",
    "<https://github.com/frankkramer-lab/riadd.aucmedi>\n",
    "\n",
    "### AUCMEDI - A Framework for Automated Classification of Medical Images\n",
    "<https://pypi.org/project/aucmedi/>\n",
    "\n",
    "### RETINAL FUNDUS MULTI-DISEASE IMAGE DATASET (RFMID)\n",
    "<https://ieee-dataport.org/open-access/retinal-fundus-multi-disease-image-dataset-rfmid#files>\n",
    "\n",
    "### RFMiD Train Dataset - kaggle\n",
    "<https://www.kaggle.com/awsaf49/rfmid-train-dataset>\n",
    "\n",
    "### Retinal Image Analysis for multi-Disease Detection Challenge website\n",
    "<https://riadd.grand-challenge.org/>\n",
    "\n",
    "### IEEE ISBI 2021 International Symposium on Biomedical Imaging April 13-16 2021\n",
    "<https://biomedicalimaging.org/2021/>\n",
    "\n",
    "---\n",
    "\n",
    "### sklearn utils class_weight.py - GitHub\n",
    "<https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/utils/class_weight.py>\n",
    "\n",
    "### sklearn.preprocessing.LabelEncoder\n",
    "<https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html>\n",
    "\n",
    "### sklearn.utils.class_weight.compute_class_weight\n",
    "<https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html>\n",
    "\n",
    "---\n",
    "\n",
    "### \n",
    "<>\n",
    "\n",
    "### \n",
    "<>\n",
    "\n",
    "### \n",
    "<>\n",
    "\n",
    "### \n",
    "<>\n",
    "\n",
    "### \n",
    "<>\n",
    "\n",
    "### \n",
    "<>\n",
    "\n",
    "### \n",
    "<>\n",
    "\n",
    "### \n",
    "<>\n",
    "\n",
    "### \n",
    "<>\n",
    "\n",
    "### \n",
    "<>\n",
    "\n",
    "### \n",
    "<>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Austin Powers - Live dangerously meme 1\n",
    "<https://i.kym-cdn.com/photos/images/newsfeed/000/511/991/3a5.jpg>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "198.196px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
